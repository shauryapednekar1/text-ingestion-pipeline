{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Goal \u00b6 The goal of this project was to design and implement a python package that lets the user easily ingest a textual dataset into a vectorstore. Priorities \u00b6 Convenience: Ingest a dataset in just a few lines of code! Performance: Use multiprocessing where applicable to fully leverage your compute's capabilities. [^1] Customizability: Easily override the default loading, chunking, and embedding functions. Modularity: Use the loader, chunker, and embedder functionality separately if needed. Extendability: Heavy documentation and tests. Key design principles Avoid re-inventing the wheel: Leveraged open source libraries (mainly LangChain[^2]). DRY (Don't Repeat Yourself): Organized code in order to provide modularity and prevent duplication. Design Overview \u00b6 Class Structure \u00b6 Here's the high level overview of the classes and how they interact with each other. sequenceDiagram participant I as Ingester participant L as Loader participant C as Chunker participant E as Embedder I->>+L: initiate loading L-->>-I: return EnhancedDocuments I->>+C: send documents for chunking C-->>-I: return chunked documents I->>+E: send chunks for embedding E-->>-I: return embedded documents E->>E: save embedded documents to vectorstore (optional) Note right of I: Ingester coordinates all interactions and manages workflow While the right level of abstraction when ingesting textual data into a vectorstore is somewhat subjective, three distinct steps stand out: Standardizing the input dataset. Chunking the standardized data. Embedding the chunked data. Additionally, we make the assumption that the usual access pattern here is that we would want to use the same loading, chunking, and embedding mechanism across multiple datasets within a given application, since this would provide consistency for downstream applications. With this assumption, it makes sense to have a Loader , Chunker , and Embedder class. Each instance of the class would share state information like how it should load, chunk, and embed data. We also have an Ingester class, which is responsible for transferring data through the instances of the three classes mentioned earlier. Another layer of abstraction that proves useful is that of a EnhancedDocument [^3], which is essentially a piece of text accompanied by some additional information. The key information any Document must have is the following: a. source : the path of the file. We assume the dataset is static (i.e. the raw data does not change). b. page_content : the actual text of the file. c. metadata : additional information about the document. Often useful for querying within the context of knowledge graphs. d. document_hash , content_hash , metadata_hash : hashes of the content, metadata, and overall document. Useful when checking uniqueness. Since there is a one-to-many relationship between a EnhancedDocument and its chunks - the chunk retains the original document's source and metadata - the type of a chunk is the same as the type of an \"unchunked\" document. Hence, the package treats chunks data as EnhancedDocuments too. Each class performs actions related to its position in the ingestion pipeline. The unit of information being transferred between classes is an EnhancedDocument . Misc Implementation Details \u00b6 For performance reasons, I've tried to use iterators and batching where possible, both to leverage vectorization and to be more space efficient. For its first iteration, this package does not try to implement upserts/idempotence, nor does it clean indexes (this is the main purpose of LangChain's Indexing API, but I didn't get the time to implement this). I did not try to optimize the package size/dependencies used - I figured this would not be the limiting factor here. Possible Enhancements \u00b6 Given that my goal was to spend less than a week working on this, many tradeoffs were made. I'll highlight the main ones here. Testing \u00b6 Due to time constraints, I focused on getting the package to a functional state first. This package contains a few unit tests for the Loader and Chunker classes, but they are by no means exhaustive. I plan on adding more unit tests in the future. For integration tests, a few scenarios I think would be worth testing are: Running the ingestion pipeline on the same dataset twice using the same vectorstore [^4]. Create a sample dataset of a few sentences that have clear and distinct categories. Ingest this into a vectorstore and perform similarity search, verifying that it returns the expected results. Customizability \u00b6 Providing more defaults for the Document Loaders, Embedders, and Vectorstores. Also, providing the ability to easily select a relevance score for the vectorstore would be nice. Performance \u00b6 This package doesn't leverage distributed computing/multiple GPUs efficiently right now. It might be worth sharding the dataset and parallelizing the work across available GPUs, or using something like Ray to do this ( example ). Footnotes \u00b6 [^1]: More work can be done to allow this package to fully leverage multiple GPUs. See the Future Work section. [^2]: However, I have come to realize that the convencience provided by LangChain's wrappers are sometimes not worth the limitations it enforces :) . [^3]: The package doesn't use the name Document since LangChain has claimed that name. Also, LangChain has a _HashedDocument class that offers similar functionality, but since its a private class, it isn't used by this package. [^4]: This currently fails due to limitations with Faiss' implementation. Fixing this either requires more custom logic for upserts, or switching to using LangChain's Indexing API. See Faiss' default configuration in defaults.py for more.","title":"Home"},{"location":"#goal","text":"The goal of this project was to design and implement a python package that lets the user easily ingest a textual dataset into a vectorstore.","title":"Goal"},{"location":"#priorities","text":"Convenience: Ingest a dataset in just a few lines of code! Performance: Use multiprocessing where applicable to fully leverage your compute's capabilities. [^1] Customizability: Easily override the default loading, chunking, and embedding functions. Modularity: Use the loader, chunker, and embedder functionality separately if needed. Extendability: Heavy documentation and tests. Key design principles Avoid re-inventing the wheel: Leveraged open source libraries (mainly LangChain[^2]). DRY (Don't Repeat Yourself): Organized code in order to provide modularity and prevent duplication.","title":"Priorities"},{"location":"#design-overview","text":"","title":"Design Overview"},{"location":"#class-structure","text":"Here's the high level overview of the classes and how they interact with each other. sequenceDiagram participant I as Ingester participant L as Loader participant C as Chunker participant E as Embedder I->>+L: initiate loading L-->>-I: return EnhancedDocuments I->>+C: send documents for chunking C-->>-I: return chunked documents I->>+E: send chunks for embedding E-->>-I: return embedded documents E->>E: save embedded documents to vectorstore (optional) Note right of I: Ingester coordinates all interactions and manages workflow While the right level of abstraction when ingesting textual data into a vectorstore is somewhat subjective, three distinct steps stand out: Standardizing the input dataset. Chunking the standardized data. Embedding the chunked data. Additionally, we make the assumption that the usual access pattern here is that we would want to use the same loading, chunking, and embedding mechanism across multiple datasets within a given application, since this would provide consistency for downstream applications. With this assumption, it makes sense to have a Loader , Chunker , and Embedder class. Each instance of the class would share state information like how it should load, chunk, and embed data. We also have an Ingester class, which is responsible for transferring data through the instances of the three classes mentioned earlier. Another layer of abstraction that proves useful is that of a EnhancedDocument [^3], which is essentially a piece of text accompanied by some additional information. The key information any Document must have is the following: a. source : the path of the file. We assume the dataset is static (i.e. the raw data does not change). b. page_content : the actual text of the file. c. metadata : additional information about the document. Often useful for querying within the context of knowledge graphs. d. document_hash , content_hash , metadata_hash : hashes of the content, metadata, and overall document. Useful when checking uniqueness. Since there is a one-to-many relationship between a EnhancedDocument and its chunks - the chunk retains the original document's source and metadata - the type of a chunk is the same as the type of an \"unchunked\" document. Hence, the package treats chunks data as EnhancedDocuments too. Each class performs actions related to its position in the ingestion pipeline. The unit of information being transferred between classes is an EnhancedDocument .","title":"Class Structure"},{"location":"#misc-implementation-details","text":"For performance reasons, I've tried to use iterators and batching where possible, both to leverage vectorization and to be more space efficient. For its first iteration, this package does not try to implement upserts/idempotence, nor does it clean indexes (this is the main purpose of LangChain's Indexing API, but I didn't get the time to implement this). I did not try to optimize the package size/dependencies used - I figured this would not be the limiting factor here.","title":"Misc Implementation Details"},{"location":"#possible-enhancements","text":"Given that my goal was to spend less than a week working on this, many tradeoffs were made. I'll highlight the main ones here.","title":"Possible Enhancements"},{"location":"#testing","text":"Due to time constraints, I focused on getting the package to a functional state first. This package contains a few unit tests for the Loader and Chunker classes, but they are by no means exhaustive. I plan on adding more unit tests in the future. For integration tests, a few scenarios I think would be worth testing are: Running the ingestion pipeline on the same dataset twice using the same vectorstore [^4]. Create a sample dataset of a few sentences that have clear and distinct categories. Ingest this into a vectorstore and perform similarity search, verifying that it returns the expected results.","title":"Testing"},{"location":"#customizability","text":"Providing more defaults for the Document Loaders, Embedders, and Vectorstores. Also, providing the ability to easily select a relevance score for the vectorstore would be nice.","title":"Customizability"},{"location":"#performance","text":"This package doesn't leverage distributed computing/multiple GPUs efficiently right now. It might be worth sharding the dataset and parallelizing the work across available GPUs, or using something like Ray to do this ( example ).","title":"Performance"},{"location":"#footnotes","text":"[^1]: More work can be done to allow this package to fully leverage multiple GPUs. See the Future Work section. [^2]: However, I have come to realize that the convencience provided by LangChain's wrappers are sometimes not worth the limitations it enforces :) . [^3]: The package doesn't use the name Document since LangChain has claimed that name. Also, LangChain has a _HashedDocument class that offers similar functionality, but since its a private class, it isn't used by this package. [^4]: This currently fails due to limitations with Faiss' implementation. Fixing this either requires more custom logic for upserts, or switching to using LangChain's Indexing API. See Faiss' default configuration in defaults.py for more.","title":"Footnotes"},{"location":"api-reference/chunk/","text":"Chunker API Reference \u00b6 Chunker \u00b6 Chunk documents. Potential functions to override if implementing a custom Chunker class: - chunk_docs() : the logic for how a document is is chunked. - get_splitter() : the logic for which splitter to use. Source code in src/chunk.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class Chunker : \"\"\"Chunk documents. Potential functions to override if implementing a custom Chunker class: - `chunk_docs()`: the logic for how a document is is chunked. - `get_splitter()`: the logic for which splitter to use. \"\"\" ALLOWED_SPLITTERS = { \"custom\" , \"recursive\" } def __init__ ( self , splitter : str = \"recursive\" , splitter_config : Dict = DEFAULT_SPLITTERS_CONFIG , num_workers : int = 10 , ) -> None : \"\"\" Initializes the Chunker instance with specified splitter configuration and multiprocessing settings. Args: splitter (str): The name of the splitter to use. Currently supports 'custom' or 'recursive'. splitter_config (Dict): Configuration dictionary for the chosen splitter. num_workers (int): Number of worker processes to use for chunking. Raises: ValueError: If the specified splitter is not supported. \"\"\" self . splitter_config = splitter_config self . num_workers = num_workers if splitter not in self . ALLOWED_SPLITTERS : raise ValueError ( f \" { splitter } is not a valid splitter.\" f \" Choose from: { self . ALLOWED_SPLITTERS } \" ) self . splitter = self . get_splitter ( splitter ) def chunk_dataset ( self , input_dir : str , detailed_progress : bool = False , output_dir : Optional [ str ] = None , num_workers : Optional [ int ] = None , ) -> None : \"\"\" Processes and chunks all documents in a specified directory. Args: input_dir (str): Directory containing the documents to be chunked. output_dir (str, optional): Directory to save the chunked documents if save_chunks is True. detailed_progress (bool): Whether to show detailed progress during the chunking process. num_workers (int, optional): Number of worker processes to use; defaults to the instance's num_workers if not provided. Raises: ValueError: If `save_chunks` is True but `output_dir` is not provided. \"\"\" if num_workers is None : num_workers = self . num_workers num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( input_dir ))) partial_func = partial ( self . chunk_file , save_chunks = True , output_dir = output_dir ) with tqdm ( total = num_files , desc = \"Chunking files\" , unit = \" files\" , smoothing = 0 ) as pbar : with multiprocessing . Pool ( num_workers ) as pool : for _ in pool . imap_unordered ( partial_func , get_files_from_dir ( input_dir ), ): pbar . update ( 1 ) def chunk_file ( self , save_chunks : bool , output_dir : Optional [ str ], file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Chunks a single file into smaller EnhancedDocuments. Args: save_chunks (bool): Whether to save the chunked documents. output_dir (str): Directory to save the documents if save_chunks is True. file_path (str): Path to the file to be chunked. Returns: List[EnhancedDocument]: A list of chunked documents. \"\"\" logging . debug ( \"Chunking file: %s \" , file_path ) raw_docs = load_docs_from_jsonl ( file_path ) chunked_docs = self . chunk_docs ( raw_docs ) if save_chunks : if output_dir is None : raise ValueError ( \"Must provide an output directory when saving documents.\" ) save_docs_to_file ( chunked_docs , file_path , output_dir ) logging . debug ( \"Chunked file: %s \" , file_path ) return chunked_docs def chunk_docs ( self , raw_docs : List [ EnhancedDocument ] ) -> List [ EnhancedDocument ]: \"\"\" Splits a list of EnhancedDocuments into smaller, chunked EnhancedDocuments. Args: raw_docs (List[EnhancedDocument]): List of documents to be chunked. Returns: List[EnhancedDocument]: A list of chunked documents. \"\"\" chunked_docs = self . splitter . split_documents ( raw_docs ) # NOTE(STP): We need to remove the hashes here since the page content # for the chunk differs from the parent document. docs = [ EnhancedDocument . remove_hashes ( doc ) for doc in chunked_docs ] docs = [ EnhancedDocument . from_document ( doc ) for doc in docs ] return docs def get_splitter ( self , splitter : str ) -> TextSplitter : \"\"\" Retrieves the appropriate document splitter based on the specified type. Args: splitter (str): The name of the splitter to use. Returns: TextSplitter: An instance of a TextSplitter. Raises: NotImplementedError: If a 'custom' splitter is specified but not implemented. ValueError: If the specified splitter type is not recognized. \"\"\" if splitter == \"custom\" : error_message = \"\"\" \"If using custom vectorstore, the Embedder.set_vectorstore() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) if splitter == \"recursive\" : kwargs = self . splitter_config [ \"recursive\" ] return RecursiveCharacterTextSplitter ( ** kwargs ) else : raise ValueError ( \"Splitter not recognized: %s \" , splitter ) __init__ ( splitter = 'recursive' , splitter_config = DEFAULT_SPLITTERS_CONFIG , num_workers = 10 ) \u00b6 Initializes the Chunker instance with specified splitter configuration and multiprocessing settings. Parameters: splitter ( str , default: 'recursive' ) \u2013 The name of the splitter to use. Currently supports 'custom' or 'recursive'. splitter_config ( Dict , default: DEFAULT_SPLITTERS_CONFIG ) \u2013 Configuration dictionary for the chosen splitter. num_workers ( int , default: 10 ) \u2013 Number of worker processes to use for chunking. Raises: ValueError \u2013 If the specified splitter is not supported. Source code in src/chunk.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , splitter : str = \"recursive\" , splitter_config : Dict = DEFAULT_SPLITTERS_CONFIG , num_workers : int = 10 , ) -> None : \"\"\" Initializes the Chunker instance with specified splitter configuration and multiprocessing settings. Args: splitter (str): The name of the splitter to use. Currently supports 'custom' or 'recursive'. splitter_config (Dict): Configuration dictionary for the chosen splitter. num_workers (int): Number of worker processes to use for chunking. Raises: ValueError: If the specified splitter is not supported. \"\"\" self . splitter_config = splitter_config self . num_workers = num_workers if splitter not in self . ALLOWED_SPLITTERS : raise ValueError ( f \" { splitter } is not a valid splitter.\" f \" Choose from: { self . ALLOWED_SPLITTERS } \" ) self . splitter = self . get_splitter ( splitter ) chunk_dataset ( input_dir , detailed_progress = False , output_dir = None , num_workers = None ) \u00b6 Processes and chunks all documents in a specified directory. Parameters: input_dir ( str ) \u2013 Directory containing the documents to be chunked. output_dir ( str , default: None ) \u2013 Directory to save the chunked documents if save_chunks is True. detailed_progress ( bool , default: False ) \u2013 Whether to show detailed progress during the chunking process. num_workers ( int , default: None ) \u2013 Number of worker processes to use; defaults to the instance's num_workers if not provided. Raises: ValueError \u2013 If save_chunks is True but output_dir is not provided. Source code in src/chunk.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def chunk_dataset ( self , input_dir : str , detailed_progress : bool = False , output_dir : Optional [ str ] = None , num_workers : Optional [ int ] = None , ) -> None : \"\"\" Processes and chunks all documents in a specified directory. Args: input_dir (str): Directory containing the documents to be chunked. output_dir (str, optional): Directory to save the chunked documents if save_chunks is True. detailed_progress (bool): Whether to show detailed progress during the chunking process. num_workers (int, optional): Number of worker processes to use; defaults to the instance's num_workers if not provided. Raises: ValueError: If `save_chunks` is True but `output_dir` is not provided. \"\"\" if num_workers is None : num_workers = self . num_workers num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( input_dir ))) partial_func = partial ( self . chunk_file , save_chunks = True , output_dir = output_dir ) with tqdm ( total = num_files , desc = \"Chunking files\" , unit = \" files\" , smoothing = 0 ) as pbar : with multiprocessing . Pool ( num_workers ) as pool : for _ in pool . imap_unordered ( partial_func , get_files_from_dir ( input_dir ), ): pbar . update ( 1 ) chunk_docs ( raw_docs ) \u00b6 Splits a list of EnhancedDocuments into smaller, chunked EnhancedDocuments. Parameters: raw_docs ( List [ EnhancedDocument ] ) \u2013 List of documents to be chunked. Returns: List [ EnhancedDocument ] \u2013 List[EnhancedDocument]: A list of chunked documents. Source code in src/chunk.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def chunk_docs ( self , raw_docs : List [ EnhancedDocument ] ) -> List [ EnhancedDocument ]: \"\"\" Splits a list of EnhancedDocuments into smaller, chunked EnhancedDocuments. Args: raw_docs (List[EnhancedDocument]): List of documents to be chunked. Returns: List[EnhancedDocument]: A list of chunked documents. \"\"\" chunked_docs = self . splitter . split_documents ( raw_docs ) # NOTE(STP): We need to remove the hashes here since the page content # for the chunk differs from the parent document. docs = [ EnhancedDocument . remove_hashes ( doc ) for doc in chunked_docs ] docs = [ EnhancedDocument . from_document ( doc ) for doc in docs ] return docs chunk_file ( save_chunks , output_dir , file_path ) \u00b6 Chunks a single file into smaller EnhancedDocuments. Parameters: save_chunks ( bool ) \u2013 Whether to save the chunked documents. output_dir ( str ) \u2013 Directory to save the documents if save_chunks is True. file_path ( str ) \u2013 Path to the file to be chunked. Returns: List [ EnhancedDocument ] \u2013 List[EnhancedDocument]: A list of chunked documents. Source code in src/chunk.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def chunk_file ( self , save_chunks : bool , output_dir : Optional [ str ], file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Chunks a single file into smaller EnhancedDocuments. Args: save_chunks (bool): Whether to save the chunked documents. output_dir (str): Directory to save the documents if save_chunks is True. file_path (str): Path to the file to be chunked. Returns: List[EnhancedDocument]: A list of chunked documents. \"\"\" logging . debug ( \"Chunking file: %s \" , file_path ) raw_docs = load_docs_from_jsonl ( file_path ) chunked_docs = self . chunk_docs ( raw_docs ) if save_chunks : if output_dir is None : raise ValueError ( \"Must provide an output directory when saving documents.\" ) save_docs_to_file ( chunked_docs , file_path , output_dir ) logging . debug ( \"Chunked file: %s \" , file_path ) return chunked_docs get_splitter ( splitter ) \u00b6 Retrieves the appropriate document splitter based on the specified type. Parameters: splitter ( str ) \u2013 The name of the splitter to use. Returns: TextSplitter ( TextSplitter ) \u2013 An instance of a TextSplitter. Raises: NotImplementedError \u2013 If a 'custom' splitter is specified but not implemented. ValueError \u2013 If the specified splitter type is not recognized. Source code in src/chunk.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def get_splitter ( self , splitter : str ) -> TextSplitter : \"\"\" Retrieves the appropriate document splitter based on the specified type. Args: splitter (str): The name of the splitter to use. Returns: TextSplitter: An instance of a TextSplitter. Raises: NotImplementedError: If a 'custom' splitter is specified but not implemented. ValueError: If the specified splitter type is not recognized. \"\"\" if splitter == \"custom\" : error_message = \"\"\" \"If using custom vectorstore, the Embedder.set_vectorstore() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) if splitter == \"recursive\" : kwargs = self . splitter_config [ \"recursive\" ] return RecursiveCharacterTextSplitter ( ** kwargs ) else : raise ValueError ( \"Splitter not recognized: %s \" , splitter )","title":"Chunker"},{"location":"api-reference/chunk/#chunker-api-reference","text":"","title":"Chunker API Reference"},{"location":"api-reference/chunk/#src.chunk.Chunker","text":"Chunk documents. Potential functions to override if implementing a custom Chunker class: - chunk_docs() : the logic for how a document is is chunked. - get_splitter() : the logic for which splitter to use. Source code in src/chunk.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 class Chunker : \"\"\"Chunk documents. Potential functions to override if implementing a custom Chunker class: - `chunk_docs()`: the logic for how a document is is chunked. - `get_splitter()`: the logic for which splitter to use. \"\"\" ALLOWED_SPLITTERS = { \"custom\" , \"recursive\" } def __init__ ( self , splitter : str = \"recursive\" , splitter_config : Dict = DEFAULT_SPLITTERS_CONFIG , num_workers : int = 10 , ) -> None : \"\"\" Initializes the Chunker instance with specified splitter configuration and multiprocessing settings. Args: splitter (str): The name of the splitter to use. Currently supports 'custom' or 'recursive'. splitter_config (Dict): Configuration dictionary for the chosen splitter. num_workers (int): Number of worker processes to use for chunking. Raises: ValueError: If the specified splitter is not supported. \"\"\" self . splitter_config = splitter_config self . num_workers = num_workers if splitter not in self . ALLOWED_SPLITTERS : raise ValueError ( f \" { splitter } is not a valid splitter.\" f \" Choose from: { self . ALLOWED_SPLITTERS } \" ) self . splitter = self . get_splitter ( splitter ) def chunk_dataset ( self , input_dir : str , detailed_progress : bool = False , output_dir : Optional [ str ] = None , num_workers : Optional [ int ] = None , ) -> None : \"\"\" Processes and chunks all documents in a specified directory. Args: input_dir (str): Directory containing the documents to be chunked. output_dir (str, optional): Directory to save the chunked documents if save_chunks is True. detailed_progress (bool): Whether to show detailed progress during the chunking process. num_workers (int, optional): Number of worker processes to use; defaults to the instance's num_workers if not provided. Raises: ValueError: If `save_chunks` is True but `output_dir` is not provided. \"\"\" if num_workers is None : num_workers = self . num_workers num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( input_dir ))) partial_func = partial ( self . chunk_file , save_chunks = True , output_dir = output_dir ) with tqdm ( total = num_files , desc = \"Chunking files\" , unit = \" files\" , smoothing = 0 ) as pbar : with multiprocessing . Pool ( num_workers ) as pool : for _ in pool . imap_unordered ( partial_func , get_files_from_dir ( input_dir ), ): pbar . update ( 1 ) def chunk_file ( self , save_chunks : bool , output_dir : Optional [ str ], file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Chunks a single file into smaller EnhancedDocuments. Args: save_chunks (bool): Whether to save the chunked documents. output_dir (str): Directory to save the documents if save_chunks is True. file_path (str): Path to the file to be chunked. Returns: List[EnhancedDocument]: A list of chunked documents. \"\"\" logging . debug ( \"Chunking file: %s \" , file_path ) raw_docs = load_docs_from_jsonl ( file_path ) chunked_docs = self . chunk_docs ( raw_docs ) if save_chunks : if output_dir is None : raise ValueError ( \"Must provide an output directory when saving documents.\" ) save_docs_to_file ( chunked_docs , file_path , output_dir ) logging . debug ( \"Chunked file: %s \" , file_path ) return chunked_docs def chunk_docs ( self , raw_docs : List [ EnhancedDocument ] ) -> List [ EnhancedDocument ]: \"\"\" Splits a list of EnhancedDocuments into smaller, chunked EnhancedDocuments. Args: raw_docs (List[EnhancedDocument]): List of documents to be chunked. Returns: List[EnhancedDocument]: A list of chunked documents. \"\"\" chunked_docs = self . splitter . split_documents ( raw_docs ) # NOTE(STP): We need to remove the hashes here since the page content # for the chunk differs from the parent document. docs = [ EnhancedDocument . remove_hashes ( doc ) for doc in chunked_docs ] docs = [ EnhancedDocument . from_document ( doc ) for doc in docs ] return docs def get_splitter ( self , splitter : str ) -> TextSplitter : \"\"\" Retrieves the appropriate document splitter based on the specified type. Args: splitter (str): The name of the splitter to use. Returns: TextSplitter: An instance of a TextSplitter. Raises: NotImplementedError: If a 'custom' splitter is specified but not implemented. ValueError: If the specified splitter type is not recognized. \"\"\" if splitter == \"custom\" : error_message = \"\"\" \"If using custom vectorstore, the Embedder.set_vectorstore() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) if splitter == \"recursive\" : kwargs = self . splitter_config [ \"recursive\" ] return RecursiveCharacterTextSplitter ( ** kwargs ) else : raise ValueError ( \"Splitter not recognized: %s \" , splitter )","title":"Chunker"},{"location":"api-reference/chunk/#src.chunk.Chunker.__init__","text":"Initializes the Chunker instance with specified splitter configuration and multiprocessing settings. Parameters: splitter ( str , default: 'recursive' ) \u2013 The name of the splitter to use. Currently supports 'custom' or 'recursive'. splitter_config ( Dict , default: DEFAULT_SPLITTERS_CONFIG ) \u2013 Configuration dictionary for the chosen splitter. num_workers ( int , default: 10 ) \u2013 Number of worker processes to use for chunking. Raises: ValueError \u2013 If the specified splitter is not supported. Source code in src/chunk.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def __init__ ( self , splitter : str = \"recursive\" , splitter_config : Dict = DEFAULT_SPLITTERS_CONFIG , num_workers : int = 10 , ) -> None : \"\"\" Initializes the Chunker instance with specified splitter configuration and multiprocessing settings. Args: splitter (str): The name of the splitter to use. Currently supports 'custom' or 'recursive'. splitter_config (Dict): Configuration dictionary for the chosen splitter. num_workers (int): Number of worker processes to use for chunking. Raises: ValueError: If the specified splitter is not supported. \"\"\" self . splitter_config = splitter_config self . num_workers = num_workers if splitter not in self . ALLOWED_SPLITTERS : raise ValueError ( f \" { splitter } is not a valid splitter.\" f \" Choose from: { self . ALLOWED_SPLITTERS } \" ) self . splitter = self . get_splitter ( splitter )","title":"__init__"},{"location":"api-reference/chunk/#src.chunk.Chunker.chunk_dataset","text":"Processes and chunks all documents in a specified directory. Parameters: input_dir ( str ) \u2013 Directory containing the documents to be chunked. output_dir ( str , default: None ) \u2013 Directory to save the chunked documents if save_chunks is True. detailed_progress ( bool , default: False ) \u2013 Whether to show detailed progress during the chunking process. num_workers ( int , default: None ) \u2013 Number of worker processes to use; defaults to the instance's num_workers if not provided. Raises: ValueError \u2013 If save_chunks is True but output_dir is not provided. Source code in src/chunk.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 def chunk_dataset ( self , input_dir : str , detailed_progress : bool = False , output_dir : Optional [ str ] = None , num_workers : Optional [ int ] = None , ) -> None : \"\"\" Processes and chunks all documents in a specified directory. Args: input_dir (str): Directory containing the documents to be chunked. output_dir (str, optional): Directory to save the chunked documents if save_chunks is True. detailed_progress (bool): Whether to show detailed progress during the chunking process. num_workers (int, optional): Number of worker processes to use; defaults to the instance's num_workers if not provided. Raises: ValueError: If `save_chunks` is True but `output_dir` is not provided. \"\"\" if num_workers is None : num_workers = self . num_workers num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( input_dir ))) partial_func = partial ( self . chunk_file , save_chunks = True , output_dir = output_dir ) with tqdm ( total = num_files , desc = \"Chunking files\" , unit = \" files\" , smoothing = 0 ) as pbar : with multiprocessing . Pool ( num_workers ) as pool : for _ in pool . imap_unordered ( partial_func , get_files_from_dir ( input_dir ), ): pbar . update ( 1 )","title":"chunk_dataset"},{"location":"api-reference/chunk/#src.chunk.Chunker.chunk_docs","text":"Splits a list of EnhancedDocuments into smaller, chunked EnhancedDocuments. Parameters: raw_docs ( List [ EnhancedDocument ] ) \u2013 List of documents to be chunked. Returns: List [ EnhancedDocument ] \u2013 List[EnhancedDocument]: A list of chunked documents. Source code in src/chunk.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def chunk_docs ( self , raw_docs : List [ EnhancedDocument ] ) -> List [ EnhancedDocument ]: \"\"\" Splits a list of EnhancedDocuments into smaller, chunked EnhancedDocuments. Args: raw_docs (List[EnhancedDocument]): List of documents to be chunked. Returns: List[EnhancedDocument]: A list of chunked documents. \"\"\" chunked_docs = self . splitter . split_documents ( raw_docs ) # NOTE(STP): We need to remove the hashes here since the page content # for the chunk differs from the parent document. docs = [ EnhancedDocument . remove_hashes ( doc ) for doc in chunked_docs ] docs = [ EnhancedDocument . from_document ( doc ) for doc in docs ] return docs","title":"chunk_docs"},{"location":"api-reference/chunk/#src.chunk.Chunker.chunk_file","text":"Chunks a single file into smaller EnhancedDocuments. Parameters: save_chunks ( bool ) \u2013 Whether to save the chunked documents. output_dir ( str ) \u2013 Directory to save the documents if save_chunks is True. file_path ( str ) \u2013 Path to the file to be chunked. Returns: List [ EnhancedDocument ] \u2013 List[EnhancedDocument]: A list of chunked documents. Source code in src/chunk.py 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def chunk_file ( self , save_chunks : bool , output_dir : Optional [ str ], file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Chunks a single file into smaller EnhancedDocuments. Args: save_chunks (bool): Whether to save the chunked documents. output_dir (str): Directory to save the documents if save_chunks is True. file_path (str): Path to the file to be chunked. Returns: List[EnhancedDocument]: A list of chunked documents. \"\"\" logging . debug ( \"Chunking file: %s \" , file_path ) raw_docs = load_docs_from_jsonl ( file_path ) chunked_docs = self . chunk_docs ( raw_docs ) if save_chunks : if output_dir is None : raise ValueError ( \"Must provide an output directory when saving documents.\" ) save_docs_to_file ( chunked_docs , file_path , output_dir ) logging . debug ( \"Chunked file: %s \" , file_path ) return chunked_docs","title":"chunk_file"},{"location":"api-reference/chunk/#src.chunk.Chunker.get_splitter","text":"Retrieves the appropriate document splitter based on the specified type. Parameters: splitter ( str ) \u2013 The name of the splitter to use. Returns: TextSplitter ( TextSplitter ) \u2013 An instance of a TextSplitter. Raises: NotImplementedError \u2013 If a 'custom' splitter is specified but not implemented. ValueError \u2013 If the specified splitter type is not recognized. Source code in src/chunk.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def get_splitter ( self , splitter : str ) -> TextSplitter : \"\"\" Retrieves the appropriate document splitter based on the specified type. Args: splitter (str): The name of the splitter to use. Returns: TextSplitter: An instance of a TextSplitter. Raises: NotImplementedError: If a 'custom' splitter is specified but not implemented. ValueError: If the specified splitter type is not recognized. \"\"\" if splitter == \"custom\" : error_message = \"\"\" \"If using custom vectorstore, the Embedder.set_vectorstore() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) if splitter == \"recursive\" : kwargs = self . splitter_config [ \"recursive\" ] return RecursiveCharacterTextSplitter ( ** kwargs ) else : raise ValueError ( \"Splitter not recognized: %s \" , splitter )","title":"get_splitter"},{"location":"api-reference/embedder/","text":"Embedder API Reference \u00b6 Embedder \u00b6 Creating embeddings for documents. Optionally store to a vectorstore. Potential functions to override if implementing a custom Embedder class: set_embedder() : the logic for how the embedder is initialized. set_vectorstore() : the logic for how the vectorstore is initialized. embed_docs() : the logic for how documents are embedded. insert_embeddings() : the logic for how embeddings are inserted into the vectorstore. Source code in src/embed.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 class Embedder : \"\"\"Creating embeddings for documents. Optionally store to a vectorstore. Potential functions to override if implementing a custom Embedder class: - `set_embedder()`: the logic for how the embedder is initialized. - `set_vectorstore()`: the logic for how the vectorstore is initialized. - `embed_docs()`: the logic for how documents are embedded. - `insert_embeddings()`: the logic for how embeddings are inserted into the vectorstore. \"\"\" ALLOWED_EMBEDDERS = { \"HuggingFace\" , \"OpenAI\" , \"custom\" } ALLOWED_VECTORSTORES = { None , \"FAISS\" , \"custom\" } def __init__ ( self , documents_dir = None , embedder = \"HuggingFace\" , embedders_config : dict = DEFAULT_EMBEDDERS_CONFIG , vectorstore : Optional [ VectorStore ] = \"FAISS\" , vectorstore_config : dict = DEFAULT_VECTORSTORES_CONFIG , ) -> None : \"\"\" Initializes an Embedder instance with specified configuration for embedding and vector storage. Args: documents_dir (str, optional): Directory containing the documents to embed. embedder (str): Type of embedder to use, options include 'HuggingFace', 'OpenAI', or 'custom'. embedders_config (dict): Configuration settings for the embedder. vectorstore (Optional[VectorStore]): Type of vector store to use, options include 'FAISS', 'custom', or None. vectorstore_config (dict): Configuration settings for the vector store. Raises: ValueError: If the specified embedder or vectorstore is not valid. \"\"\" self . documents_dir = documents_dir if embedder not in self . ALLOWED_EMBEDDERS : raise ValueError ( f \" { embedder } is not a valid embedder.\" f \" Choose from: { self . ALLOWED_EMBEDDERS } \" ) if vectorstore not in self . ALLOWED_VECTORSTORES : raise ValueError ( f \" { vectorstore } is not a valid vectorstore.\" f \" Choose from: { self . ALLOWED_VECTORSTORES } \" ) self . embedder_name : str = embedder self . embedders_config = embedders_config self . set_embedder ( embedder , config ) if vectorstore is not None : self . set_vectorstore ( vectorstore , vectorstore_config ) def embed_and_insert_dataset ( self , input_dir : str , detailed_progress : bool = False , num_workers : Optional [ int ] = None , batch_size : int = 1000 , ) -> None : \"\"\" Processes, embeds, and writes documents from the specified directory to the vectorstore in batches. Args: input_dir (str): Directory containing documents to embed. detailed_progress (bool): Whether to show detailed progress during embedding. num_workers (int, optional): Number of worker processes to use; defaults to the instance's configuration if not provided. batch_size (int): Number of files to process in each batch. Note: Uses multiprocessing to enhance performance. \"\"\" num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( input_dir ))) with tqdm ( total = num_files , desc = \"Embedding files\" , unit = \" files\" , smoothing = 0 ) as pbar : while True : file_chunk = list ( islice ( get_files_from_dir ( input_dir ), batch_size ) ) if not file_chunk : break self . embed_and_insert_files ( file_chunk ) pbar . update ( len ( file_chunk )) def embed_and_insert_files ( self , file_paths : List [ str ] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Embeds documents from specified file paths and inserts them into the vector store. Args: file_paths (List[str]): File paths to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. \"\"\" self . _verify_vectorstore_client () all_docs = [] all_embeddings = [] for file in file_paths : curr_docs , curr_embeddings = self . embed_files ( file_paths ) # NOTE(STP): We're not calling self.embed_and_insert_docs() here # in order to allow us to batch embed multiple files. all_docs . extend ( curr_docs ) all_embeddings . extend ( curr_embeddings ) docs , ids , embeddings = self . insert_embeddings ( all_docs , all_embeddings ) return ids , docs , embeddings def embed_files ( self , file_paths : List [ str ] ) -> Tuple [ EnhancedDocument , List [ List [ float ]]]: \"\"\" Embeds a batch of files specified by their paths. Args: file_paths (List[str]): List of file paths to embed. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. \"\"\" # NOTE(STP): We allow passing multiple files to take advantage of # batching benefits. logging . debug ( \"Embedding files: %s \" , file_paths ) docs = [] for file_path in file_paths : docs . extend ( load_docs_from_jsonl ( file_path )) embeddings = self . embed_docs ( docs ) logging . debug ( \"Embedded files: %s \" , file_paths ) return docs , embeddings def embed_and_insert_docs ( self , docs : List [ EnhancedDocument ] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Embeds documents and inserts their embeddings into the vectorstore, then returns the IDs, documents, and embeddings. Args: docs (List[EnhancedDocument]): Documents to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError: If the vectorstore instance is not set. \"\"\" self . _verify_vectorstore_client () embeddings = self . embed_docs ( docs ) ids , docs , embeddings = self . insert_embeddings ( docs , embeddings ) return ids , docs , embeddings def embed_docs ( self , docs : List [ EnhancedDocument ]) -> List [ List [ float ]]: \"\"\" Generates embeddings for a list of documents. Args: docs (List[EnhancedDocument]): Documents to embed. Returns: List[List[float]]: List of embeddings for each document. \"\"\" # NOTE(STP): This ignores metadata. If we want to include metadata in # the embedding, we would need to combine it with the page content # and stringify it in some manner. # TODO(STP): We might want to batch embed documents here if the number # of documents exceed a certain threshold. Would need to look more into # if and when that would be useful. logging . debug ( \"Embedding %d docs\" , len ( docs )) page_contents = [ doc . page_content for doc in docs ] embeddings = self . embedder . embed_documents ( page_contents ) logging . debug ( \"Embedded %d docs\" , len ( docs )) return embeddings def insert_embeddings ( self , docs : List [ EnhancedDocument ], embeddings : List [ List [ float ]] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Inserts the embeddings of the provided documents into the vectorstore and ensures all documents are unique based on their content hash. Args: docs (List[EnhancedDocument]): Documents whose embeddings are to be inserted. embeddings (List[List[float]]): Embeddings corresponding to the documents. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError: If the vectorstore instance is not set. \"\"\" self . _verify_vectorstore_client () logging . debug ( \"Saving %d embedded docs to vectorstore docs\" , len ( docs )) ids = [ doc . document_hash for doc in docs ] if len ( ids ) != len ( set ( ids )): # TODO(STP): Improve space efficiency here. unique_docs = [] unique_embeddings = [] unique_ids = [] seen = set () for i , curr_id in enumerate ( ids ): if curr_id in seen : logging . debug ( \"Found multiple documents from %s with the \" \" same content hash. ' %s ...'\" , docs [ i ] . metadata [ \"source\" ], docs [ i ] . page_content [: 30 ], ) else : unique_ids . append ( curr_id ) unique_docs . append ( docs [ i ]) unique_embeddings . append ( embeddings [ i ]) seen . add ( curr_id ) docs = unique_docs ids = unique_ids embeddings = unique_embeddings texts = [ doc . page_content for doc in docs ] text_embeddings = zip ( texts , embeddings ) metadatas = [ doc . metadata for doc in docs ] self . vectorstore_instance . add_embeddings ( text_embeddings = text_embeddings , ids = ids , metadatas = metadatas ) logging . debug ( \"Saved %d embedded docs to vectorstore docs\" , len ( docs )) return ids , docs , embeddings def save_vectorstore ( self ) -> None : \"\"\" Saves the current state of the vector store locally. \"\"\" if self . vectorstore_name == \"FAISS\" : save_local_config = self . vectorstore_config [ \"FAISS\" ][ \"save_local_config\" ] if save_local_config [ \"save_local\" ]: self . vectorstore_client . save_local ( save_local_config [ \"folder_path\" ], save_local_config [ \"index_name\" ], ) def set_embedder ( self , name : str , config : Dict ) -> Embeddings : \"\"\" Configures and initializes the embedder based on specified name and configuration. Args: name (str): Name of the embedder to configure. config (dict): Configuration dictionary for the embedder. Raises: NotImplementedError: If a 'custom' embedder is specified but not implemented. ValueError: If embedder name is not recognized or none provided when required. \"\"\" if name == \"custom\" : error_message = \"\"\" \"If using custom embedder, the Embedder.set_embedder() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) embedder_config = config [ name ] if name == \"OpenAI\" : return OpenAIEmbeddings ( ** embedder_config ) elif name == \"HuggingFace\" : return HuggingFaceEmbeddings ( ** embedder_config ) else : raise ValueError ( \"Embedding not recognized: %s \" , name ) def set_vectorstore ( self , name : str , config : Dict ): \"\"\" Configures and initializes the vector store based on specified name and configuration. Args: name (str): Name of the vector store to configure. config (dict): Configuration dictionary for the vector store. Raises: NotImplementedError: If a 'custom' vector store is specified but not implemented. ValueError: If vector store name is not recognized or none provided when required. \"\"\" assert name is not None and name in self . ALLOWED_VECTORSTORES if name == \"custom\" : error_message = \"\"\" \"If using custom vectorstore, the Embedder.set_vectorstore() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) self . vectorstore_name = name self . vectorstore_config = config config = config [ name ] if name == \"FAISS\" : if config [ \"load_local\" ]: load_local_config = config [ \"load_local_args\" ] load_local_config [ \"embeddings\" ] = self . embedder vectorstore_instance = FAISS . load_local ( ** load_local_config ) num_documents = len ( vectorstore_instance . index_to_docstore_id ) logging . debug ( \"Total number of documents loaded from saved FAISS \" \"vectorstore: %d \" , num_documents , ) else : config = config [ \"init_args\" ] config [ \"embedding_function\" ] = self . embedder config [ \"index\" ] = faiss . IndexFlatL2 ( self . embedder . client . get_sentence_embedding_dimension () ) config [ \"docstore\" ] = InMemoryDocstore () config [ \"index_to_docstore_id\" ] = {} vectorstore_instance = FAISS ( ** config ) self . vectorstore_instance = vectorstore_instance def _verify_vectorstore_client ( self ) -> None : \"\"\" Verifies that the vectorstore instance is properly set up. Raises: ValueError: If the vectorstore instance is not set when required. \"\"\" if self . vectorstore_instance is None : raise ValueError ( \"Vectorstore must be set when saving document embeddings.\" ) __init__ ( documents_dir = None , embedder = 'HuggingFace' , embedders_config = DEFAULT_EMBEDDERS_CONFIG , vectorstore = 'FAISS' , vectorstore_config = DEFAULT_VECTORSTORES_CONFIG ) \u00b6 Initializes an Embedder instance with specified configuration for embedding and vector storage. Parameters: documents_dir ( str , default: None ) \u2013 Directory containing the documents to embed. embedder ( str , default: 'HuggingFace' ) \u2013 Type of embedder to use, options include 'HuggingFace', 'OpenAI', or 'custom'. embedders_config ( dict , default: DEFAULT_EMBEDDERS_CONFIG ) \u2013 Configuration settings for the embedder. vectorstore ( Optional [ VectorStore ] , default: 'FAISS' ) \u2013 Type of vector store to use, options include 'FAISS', 'custom', or None. vectorstore_config ( dict , default: DEFAULT_VECTORSTORES_CONFIG ) \u2013 Configuration settings for the vector store. Raises: ValueError \u2013 If the specified embedder or vectorstore is not valid. Source code in src/embed.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , documents_dir = None , embedder = \"HuggingFace\" , embedders_config : dict = DEFAULT_EMBEDDERS_CONFIG , vectorstore : Optional [ VectorStore ] = \"FAISS\" , vectorstore_config : dict = DEFAULT_VECTORSTORES_CONFIG , ) -> None : \"\"\" Initializes an Embedder instance with specified configuration for embedding and vector storage. Args: documents_dir (str, optional): Directory containing the documents to embed. embedder (str): Type of embedder to use, options include 'HuggingFace', 'OpenAI', or 'custom'. embedders_config (dict): Configuration settings for the embedder. vectorstore (Optional[VectorStore]): Type of vector store to use, options include 'FAISS', 'custom', or None. vectorstore_config (dict): Configuration settings for the vector store. Raises: ValueError: If the specified embedder or vectorstore is not valid. \"\"\" self . documents_dir = documents_dir if embedder not in self . ALLOWED_EMBEDDERS : raise ValueError ( f \" { embedder } is not a valid embedder.\" f \" Choose from: { self . ALLOWED_EMBEDDERS } \" ) if vectorstore not in self . ALLOWED_VECTORSTORES : raise ValueError ( f \" { vectorstore } is not a valid vectorstore.\" f \" Choose from: { self . ALLOWED_VECTORSTORES } \" ) self . embedder_name : str = embedder self . embedders_config = embedders_config self . set_embedder ( embedder , config ) if vectorstore is not None : self . set_vectorstore ( vectorstore , vectorstore_config ) embed_and_insert_dataset ( input_dir , detailed_progress = False , num_workers = None , batch_size = 1000 ) \u00b6 Processes, embeds, and writes documents from the specified directory to the vectorstore in batches. Parameters: input_dir ( str ) \u2013 Directory containing documents to embed. detailed_progress ( bool , default: False ) \u2013 Whether to show detailed progress during embedding. num_workers ( int , default: None ) \u2013 Number of worker processes to use; defaults to the instance's configuration if not provided. batch_size ( int , default: 1000 ) \u2013 Number of files to process in each batch. Note Uses multiprocessing to enhance performance. Source code in src/embed.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def embed_and_insert_dataset ( self , input_dir : str , detailed_progress : bool = False , num_workers : Optional [ int ] = None , batch_size : int = 1000 , ) -> None : \"\"\" Processes, embeds, and writes documents from the specified directory to the vectorstore in batches. Args: input_dir (str): Directory containing documents to embed. detailed_progress (bool): Whether to show detailed progress during embedding. num_workers (int, optional): Number of worker processes to use; defaults to the instance's configuration if not provided. batch_size (int): Number of files to process in each batch. Note: Uses multiprocessing to enhance performance. \"\"\" num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( input_dir ))) with tqdm ( total = num_files , desc = \"Embedding files\" , unit = \" files\" , smoothing = 0 ) as pbar : while True : file_chunk = list ( islice ( get_files_from_dir ( input_dir ), batch_size ) ) if not file_chunk : break self . embed_and_insert_files ( file_chunk ) pbar . update ( len ( file_chunk )) embed_and_insert_docs ( docs ) \u00b6 Embeds documents and inserts their embeddings into the vectorstore, then returns the IDs, documents, and embeddings. Parameters: docs ( List [ EnhancedDocument ] ) \u2013 Documents to embed and insert. Returns: tuple ( Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]] ) \u2013 A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError \u2013 If the vectorstore instance is not set. Source code in src/embed.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def embed_and_insert_docs ( self , docs : List [ EnhancedDocument ] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Embeds documents and inserts their embeddings into the vectorstore, then returns the IDs, documents, and embeddings. Args: docs (List[EnhancedDocument]): Documents to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError: If the vectorstore instance is not set. \"\"\" self . _verify_vectorstore_client () embeddings = self . embed_docs ( docs ) ids , docs , embeddings = self . insert_embeddings ( docs , embeddings ) return ids , docs , embeddings embed_and_insert_files ( file_paths ) \u00b6 Embeds documents from specified file paths and inserts them into the vector store. Parameters: file_paths ( List [ str ] ) \u2013 File paths to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Source code in src/embed.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def embed_and_insert_files ( self , file_paths : List [ str ] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Embeds documents from specified file paths and inserts them into the vector store. Args: file_paths (List[str]): File paths to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. \"\"\" self . _verify_vectorstore_client () all_docs = [] all_embeddings = [] for file in file_paths : curr_docs , curr_embeddings = self . embed_files ( file_paths ) # NOTE(STP): We're not calling self.embed_and_insert_docs() here # in order to allow us to batch embed multiple files. all_docs . extend ( curr_docs ) all_embeddings . extend ( curr_embeddings ) docs , ids , embeddings = self . insert_embeddings ( all_docs , all_embeddings ) return ids , docs , embeddings embed_docs ( docs ) \u00b6 Generates embeddings for a list of documents. Parameters: docs ( List [ EnhancedDocument ] ) \u2013 Documents to embed. Returns: List [ List [ float ]] \u2013 List[List[float]]: List of embeddings for each document. Source code in src/embed.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def embed_docs ( self , docs : List [ EnhancedDocument ]) -> List [ List [ float ]]: \"\"\" Generates embeddings for a list of documents. Args: docs (List[EnhancedDocument]): Documents to embed. Returns: List[List[float]]: List of embeddings for each document. \"\"\" # NOTE(STP): This ignores metadata. If we want to include metadata in # the embedding, we would need to combine it with the page content # and stringify it in some manner. # TODO(STP): We might want to batch embed documents here if the number # of documents exceed a certain threshold. Would need to look more into # if and when that would be useful. logging . debug ( \"Embedding %d docs\" , len ( docs )) page_contents = [ doc . page_content for doc in docs ] embeddings = self . embedder . embed_documents ( page_contents ) logging . debug ( \"Embedded %d docs\" , len ( docs )) return embeddings embed_files ( file_paths ) \u00b6 Embeds a batch of files specified by their paths. Parameters: file_paths ( List [ str ] ) \u2013 List of file paths to embed. Returns: tuple ( Tuple [ EnhancedDocument , List [ List [ float ]]] ) \u2013 A tuple containing lists of ids, docs, and their embeddings. Source code in src/embed.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def embed_files ( self , file_paths : List [ str ] ) -> Tuple [ EnhancedDocument , List [ List [ float ]]]: \"\"\" Embeds a batch of files specified by their paths. Args: file_paths (List[str]): List of file paths to embed. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. \"\"\" # NOTE(STP): We allow passing multiple files to take advantage of # batching benefits. logging . debug ( \"Embedding files: %s \" , file_paths ) docs = [] for file_path in file_paths : docs . extend ( load_docs_from_jsonl ( file_path )) embeddings = self . embed_docs ( docs ) logging . debug ( \"Embedded files: %s \" , file_paths ) return docs , embeddings insert_embeddings ( docs , embeddings ) \u00b6 Inserts the embeddings of the provided documents into the vectorstore and ensures all documents are unique based on their content hash. Parameters: docs ( List [ EnhancedDocument ] ) \u2013 Documents whose embeddings are to be inserted. embeddings ( List [ List [ float ]] ) \u2013 Embeddings corresponding to the documents. Returns: tuple ( Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]] ) \u2013 A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError \u2013 If the vectorstore instance is not set. Source code in src/embed.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def insert_embeddings ( self , docs : List [ EnhancedDocument ], embeddings : List [ List [ float ]] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Inserts the embeddings of the provided documents into the vectorstore and ensures all documents are unique based on their content hash. Args: docs (List[EnhancedDocument]): Documents whose embeddings are to be inserted. embeddings (List[List[float]]): Embeddings corresponding to the documents. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError: If the vectorstore instance is not set. \"\"\" self . _verify_vectorstore_client () logging . debug ( \"Saving %d embedded docs to vectorstore docs\" , len ( docs )) ids = [ doc . document_hash for doc in docs ] if len ( ids ) != len ( set ( ids )): # TODO(STP): Improve space efficiency here. unique_docs = [] unique_embeddings = [] unique_ids = [] seen = set () for i , curr_id in enumerate ( ids ): if curr_id in seen : logging . debug ( \"Found multiple documents from %s with the \" \" same content hash. ' %s ...'\" , docs [ i ] . metadata [ \"source\" ], docs [ i ] . page_content [: 30 ], ) else : unique_ids . append ( curr_id ) unique_docs . append ( docs [ i ]) unique_embeddings . append ( embeddings [ i ]) seen . add ( curr_id ) docs = unique_docs ids = unique_ids embeddings = unique_embeddings texts = [ doc . page_content for doc in docs ] text_embeddings = zip ( texts , embeddings ) metadatas = [ doc . metadata for doc in docs ] self . vectorstore_instance . add_embeddings ( text_embeddings = text_embeddings , ids = ids , metadatas = metadatas ) logging . debug ( \"Saved %d embedded docs to vectorstore docs\" , len ( docs )) return ids , docs , embeddings save_vectorstore () \u00b6 Saves the current state of the vector store locally. Source code in src/embed.py 274 275 276 277 278 279 280 281 282 283 284 285 286 def save_vectorstore ( self ) -> None : \"\"\" Saves the current state of the vector store locally. \"\"\" if self . vectorstore_name == \"FAISS\" : save_local_config = self . vectorstore_config [ \"FAISS\" ][ \"save_local_config\" ] if save_local_config [ \"save_local\" ]: self . vectorstore_client . save_local ( save_local_config [ \"folder_path\" ], save_local_config [ \"index_name\" ], ) set_embedder ( name , config ) \u00b6 Configures and initializes the embedder based on specified name and configuration. Parameters: name ( str ) \u2013 Name of the embedder to configure. config ( dict ) \u2013 Configuration dictionary for the embedder. Raises: NotImplementedError \u2013 If a 'custom' embedder is specified but not implemented. ValueError \u2013 If embedder name is not recognized or none provided when required. Source code in src/embed.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def set_embedder ( self , name : str , config : Dict ) -> Embeddings : \"\"\" Configures and initializes the embedder based on specified name and configuration. Args: name (str): Name of the embedder to configure. config (dict): Configuration dictionary for the embedder. Raises: NotImplementedError: If a 'custom' embedder is specified but not implemented. ValueError: If embedder name is not recognized or none provided when required. \"\"\" if name == \"custom\" : error_message = \"\"\" \"If using custom embedder, the Embedder.set_embedder() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) embedder_config = config [ name ] if name == \"OpenAI\" : return OpenAIEmbeddings ( ** embedder_config ) elif name == \"HuggingFace\" : return HuggingFaceEmbeddings ( ** embedder_config ) else : raise ValueError ( \"Embedding not recognized: %s \" , name ) set_vectorstore ( name , config ) \u00b6 Configures and initializes the vector store based on specified name and configuration. Parameters: name ( str ) \u2013 Name of the vector store to configure. config ( dict ) \u2013 Configuration dictionary for the vector store. Raises: NotImplementedError \u2013 If a 'custom' vector store is specified but not implemented. ValueError \u2013 If vector store name is not recognized or none provided when required. Source code in src/embed.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def set_vectorstore ( self , name : str , config : Dict ): \"\"\" Configures and initializes the vector store based on specified name and configuration. Args: name (str): Name of the vector store to configure. config (dict): Configuration dictionary for the vector store. Raises: NotImplementedError: If a 'custom' vector store is specified but not implemented. ValueError: If vector store name is not recognized or none provided when required. \"\"\" assert name is not None and name in self . ALLOWED_VECTORSTORES if name == \"custom\" : error_message = \"\"\" \"If using custom vectorstore, the Embedder.set_vectorstore() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) self . vectorstore_name = name self . vectorstore_config = config config = config [ name ] if name == \"FAISS\" : if config [ \"load_local\" ]: load_local_config = config [ \"load_local_args\" ] load_local_config [ \"embeddings\" ] = self . embedder vectorstore_instance = FAISS . load_local ( ** load_local_config ) num_documents = len ( vectorstore_instance . index_to_docstore_id ) logging . debug ( \"Total number of documents loaded from saved FAISS \" \"vectorstore: %d \" , num_documents , ) else : config = config [ \"init_args\" ] config [ \"embedding_function\" ] = self . embedder config [ \"index\" ] = faiss . IndexFlatL2 ( self . embedder . client . get_sentence_embedding_dimension () ) config [ \"docstore\" ] = InMemoryDocstore () config [ \"index_to_docstore_id\" ] = {} vectorstore_instance = FAISS ( ** config ) self . vectorstore_instance = vectorstore_instance","title":"Embedder"},{"location":"api-reference/embedder/#embedder-api-reference","text":"","title":"Embedder API Reference"},{"location":"api-reference/embedder/#src.embed.Embedder","text":"Creating embeddings for documents. Optionally store to a vectorstore. Potential functions to override if implementing a custom Embedder class: set_embedder() : the logic for how the embedder is initialized. set_vectorstore() : the logic for how the vectorstore is initialized. embed_docs() : the logic for how documents are embedded. insert_embeddings() : the logic for how embeddings are inserted into the vectorstore. Source code in src/embed.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 class Embedder : \"\"\"Creating embeddings for documents. Optionally store to a vectorstore. Potential functions to override if implementing a custom Embedder class: - `set_embedder()`: the logic for how the embedder is initialized. - `set_vectorstore()`: the logic for how the vectorstore is initialized. - `embed_docs()`: the logic for how documents are embedded. - `insert_embeddings()`: the logic for how embeddings are inserted into the vectorstore. \"\"\" ALLOWED_EMBEDDERS = { \"HuggingFace\" , \"OpenAI\" , \"custom\" } ALLOWED_VECTORSTORES = { None , \"FAISS\" , \"custom\" } def __init__ ( self , documents_dir = None , embedder = \"HuggingFace\" , embedders_config : dict = DEFAULT_EMBEDDERS_CONFIG , vectorstore : Optional [ VectorStore ] = \"FAISS\" , vectorstore_config : dict = DEFAULT_VECTORSTORES_CONFIG , ) -> None : \"\"\" Initializes an Embedder instance with specified configuration for embedding and vector storage. Args: documents_dir (str, optional): Directory containing the documents to embed. embedder (str): Type of embedder to use, options include 'HuggingFace', 'OpenAI', or 'custom'. embedders_config (dict): Configuration settings for the embedder. vectorstore (Optional[VectorStore]): Type of vector store to use, options include 'FAISS', 'custom', or None. vectorstore_config (dict): Configuration settings for the vector store. Raises: ValueError: If the specified embedder or vectorstore is not valid. \"\"\" self . documents_dir = documents_dir if embedder not in self . ALLOWED_EMBEDDERS : raise ValueError ( f \" { embedder } is not a valid embedder.\" f \" Choose from: { self . ALLOWED_EMBEDDERS } \" ) if vectorstore not in self . ALLOWED_VECTORSTORES : raise ValueError ( f \" { vectorstore } is not a valid vectorstore.\" f \" Choose from: { self . ALLOWED_VECTORSTORES } \" ) self . embedder_name : str = embedder self . embedders_config = embedders_config self . set_embedder ( embedder , config ) if vectorstore is not None : self . set_vectorstore ( vectorstore , vectorstore_config ) def embed_and_insert_dataset ( self , input_dir : str , detailed_progress : bool = False , num_workers : Optional [ int ] = None , batch_size : int = 1000 , ) -> None : \"\"\" Processes, embeds, and writes documents from the specified directory to the vectorstore in batches. Args: input_dir (str): Directory containing documents to embed. detailed_progress (bool): Whether to show detailed progress during embedding. num_workers (int, optional): Number of worker processes to use; defaults to the instance's configuration if not provided. batch_size (int): Number of files to process in each batch. Note: Uses multiprocessing to enhance performance. \"\"\" num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( input_dir ))) with tqdm ( total = num_files , desc = \"Embedding files\" , unit = \" files\" , smoothing = 0 ) as pbar : while True : file_chunk = list ( islice ( get_files_from_dir ( input_dir ), batch_size ) ) if not file_chunk : break self . embed_and_insert_files ( file_chunk ) pbar . update ( len ( file_chunk )) def embed_and_insert_files ( self , file_paths : List [ str ] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Embeds documents from specified file paths and inserts them into the vector store. Args: file_paths (List[str]): File paths to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. \"\"\" self . _verify_vectorstore_client () all_docs = [] all_embeddings = [] for file in file_paths : curr_docs , curr_embeddings = self . embed_files ( file_paths ) # NOTE(STP): We're not calling self.embed_and_insert_docs() here # in order to allow us to batch embed multiple files. all_docs . extend ( curr_docs ) all_embeddings . extend ( curr_embeddings ) docs , ids , embeddings = self . insert_embeddings ( all_docs , all_embeddings ) return ids , docs , embeddings def embed_files ( self , file_paths : List [ str ] ) -> Tuple [ EnhancedDocument , List [ List [ float ]]]: \"\"\" Embeds a batch of files specified by their paths. Args: file_paths (List[str]): List of file paths to embed. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. \"\"\" # NOTE(STP): We allow passing multiple files to take advantage of # batching benefits. logging . debug ( \"Embedding files: %s \" , file_paths ) docs = [] for file_path in file_paths : docs . extend ( load_docs_from_jsonl ( file_path )) embeddings = self . embed_docs ( docs ) logging . debug ( \"Embedded files: %s \" , file_paths ) return docs , embeddings def embed_and_insert_docs ( self , docs : List [ EnhancedDocument ] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Embeds documents and inserts their embeddings into the vectorstore, then returns the IDs, documents, and embeddings. Args: docs (List[EnhancedDocument]): Documents to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError: If the vectorstore instance is not set. \"\"\" self . _verify_vectorstore_client () embeddings = self . embed_docs ( docs ) ids , docs , embeddings = self . insert_embeddings ( docs , embeddings ) return ids , docs , embeddings def embed_docs ( self , docs : List [ EnhancedDocument ]) -> List [ List [ float ]]: \"\"\" Generates embeddings for a list of documents. Args: docs (List[EnhancedDocument]): Documents to embed. Returns: List[List[float]]: List of embeddings for each document. \"\"\" # NOTE(STP): This ignores metadata. If we want to include metadata in # the embedding, we would need to combine it with the page content # and stringify it in some manner. # TODO(STP): We might want to batch embed documents here if the number # of documents exceed a certain threshold. Would need to look more into # if and when that would be useful. logging . debug ( \"Embedding %d docs\" , len ( docs )) page_contents = [ doc . page_content for doc in docs ] embeddings = self . embedder . embed_documents ( page_contents ) logging . debug ( \"Embedded %d docs\" , len ( docs )) return embeddings def insert_embeddings ( self , docs : List [ EnhancedDocument ], embeddings : List [ List [ float ]] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Inserts the embeddings of the provided documents into the vectorstore and ensures all documents are unique based on their content hash. Args: docs (List[EnhancedDocument]): Documents whose embeddings are to be inserted. embeddings (List[List[float]]): Embeddings corresponding to the documents. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError: If the vectorstore instance is not set. \"\"\" self . _verify_vectorstore_client () logging . debug ( \"Saving %d embedded docs to vectorstore docs\" , len ( docs )) ids = [ doc . document_hash for doc in docs ] if len ( ids ) != len ( set ( ids )): # TODO(STP): Improve space efficiency here. unique_docs = [] unique_embeddings = [] unique_ids = [] seen = set () for i , curr_id in enumerate ( ids ): if curr_id in seen : logging . debug ( \"Found multiple documents from %s with the \" \" same content hash. ' %s ...'\" , docs [ i ] . metadata [ \"source\" ], docs [ i ] . page_content [: 30 ], ) else : unique_ids . append ( curr_id ) unique_docs . append ( docs [ i ]) unique_embeddings . append ( embeddings [ i ]) seen . add ( curr_id ) docs = unique_docs ids = unique_ids embeddings = unique_embeddings texts = [ doc . page_content for doc in docs ] text_embeddings = zip ( texts , embeddings ) metadatas = [ doc . metadata for doc in docs ] self . vectorstore_instance . add_embeddings ( text_embeddings = text_embeddings , ids = ids , metadatas = metadatas ) logging . debug ( \"Saved %d embedded docs to vectorstore docs\" , len ( docs )) return ids , docs , embeddings def save_vectorstore ( self ) -> None : \"\"\" Saves the current state of the vector store locally. \"\"\" if self . vectorstore_name == \"FAISS\" : save_local_config = self . vectorstore_config [ \"FAISS\" ][ \"save_local_config\" ] if save_local_config [ \"save_local\" ]: self . vectorstore_client . save_local ( save_local_config [ \"folder_path\" ], save_local_config [ \"index_name\" ], ) def set_embedder ( self , name : str , config : Dict ) -> Embeddings : \"\"\" Configures and initializes the embedder based on specified name and configuration. Args: name (str): Name of the embedder to configure. config (dict): Configuration dictionary for the embedder. Raises: NotImplementedError: If a 'custom' embedder is specified but not implemented. ValueError: If embedder name is not recognized or none provided when required. \"\"\" if name == \"custom\" : error_message = \"\"\" \"If using custom embedder, the Embedder.set_embedder() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) embedder_config = config [ name ] if name == \"OpenAI\" : return OpenAIEmbeddings ( ** embedder_config ) elif name == \"HuggingFace\" : return HuggingFaceEmbeddings ( ** embedder_config ) else : raise ValueError ( \"Embedding not recognized: %s \" , name ) def set_vectorstore ( self , name : str , config : Dict ): \"\"\" Configures and initializes the vector store based on specified name and configuration. Args: name (str): Name of the vector store to configure. config (dict): Configuration dictionary for the vector store. Raises: NotImplementedError: If a 'custom' vector store is specified but not implemented. ValueError: If vector store name is not recognized or none provided when required. \"\"\" assert name is not None and name in self . ALLOWED_VECTORSTORES if name == \"custom\" : error_message = \"\"\" \"If using custom vectorstore, the Embedder.set_vectorstore() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) self . vectorstore_name = name self . vectorstore_config = config config = config [ name ] if name == \"FAISS\" : if config [ \"load_local\" ]: load_local_config = config [ \"load_local_args\" ] load_local_config [ \"embeddings\" ] = self . embedder vectorstore_instance = FAISS . load_local ( ** load_local_config ) num_documents = len ( vectorstore_instance . index_to_docstore_id ) logging . debug ( \"Total number of documents loaded from saved FAISS \" \"vectorstore: %d \" , num_documents , ) else : config = config [ \"init_args\" ] config [ \"embedding_function\" ] = self . embedder config [ \"index\" ] = faiss . IndexFlatL2 ( self . embedder . client . get_sentence_embedding_dimension () ) config [ \"docstore\" ] = InMemoryDocstore () config [ \"index_to_docstore_id\" ] = {} vectorstore_instance = FAISS ( ** config ) self . vectorstore_instance = vectorstore_instance def _verify_vectorstore_client ( self ) -> None : \"\"\" Verifies that the vectorstore instance is properly set up. Raises: ValueError: If the vectorstore instance is not set when required. \"\"\" if self . vectorstore_instance is None : raise ValueError ( \"Vectorstore must be set when saving document embeddings.\" )","title":"Embedder"},{"location":"api-reference/embedder/#src.embed.Embedder.__init__","text":"Initializes an Embedder instance with specified configuration for embedding and vector storage. Parameters: documents_dir ( str , default: None ) \u2013 Directory containing the documents to embed. embedder ( str , default: 'HuggingFace' ) \u2013 Type of embedder to use, options include 'HuggingFace', 'OpenAI', or 'custom'. embedders_config ( dict , default: DEFAULT_EMBEDDERS_CONFIG ) \u2013 Configuration settings for the embedder. vectorstore ( Optional [ VectorStore ] , default: 'FAISS' ) \u2013 Type of vector store to use, options include 'FAISS', 'custom', or None. vectorstore_config ( dict , default: DEFAULT_VECTORSTORES_CONFIG ) \u2013 Configuration settings for the vector store. Raises: ValueError \u2013 If the specified embedder or vectorstore is not valid. Source code in src/embed.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def __init__ ( self , documents_dir = None , embedder = \"HuggingFace\" , embedders_config : dict = DEFAULT_EMBEDDERS_CONFIG , vectorstore : Optional [ VectorStore ] = \"FAISS\" , vectorstore_config : dict = DEFAULT_VECTORSTORES_CONFIG , ) -> None : \"\"\" Initializes an Embedder instance with specified configuration for embedding and vector storage. Args: documents_dir (str, optional): Directory containing the documents to embed. embedder (str): Type of embedder to use, options include 'HuggingFace', 'OpenAI', or 'custom'. embedders_config (dict): Configuration settings for the embedder. vectorstore (Optional[VectorStore]): Type of vector store to use, options include 'FAISS', 'custom', or None. vectorstore_config (dict): Configuration settings for the vector store. Raises: ValueError: If the specified embedder or vectorstore is not valid. \"\"\" self . documents_dir = documents_dir if embedder not in self . ALLOWED_EMBEDDERS : raise ValueError ( f \" { embedder } is not a valid embedder.\" f \" Choose from: { self . ALLOWED_EMBEDDERS } \" ) if vectorstore not in self . ALLOWED_VECTORSTORES : raise ValueError ( f \" { vectorstore } is not a valid vectorstore.\" f \" Choose from: { self . ALLOWED_VECTORSTORES } \" ) self . embedder_name : str = embedder self . embedders_config = embedders_config self . set_embedder ( embedder , config ) if vectorstore is not None : self . set_vectorstore ( vectorstore , vectorstore_config )","title":"__init__"},{"location":"api-reference/embedder/#src.embed.Embedder.embed_and_insert_dataset","text":"Processes, embeds, and writes documents from the specified directory to the vectorstore in batches. Parameters: input_dir ( str ) \u2013 Directory containing documents to embed. detailed_progress ( bool , default: False ) \u2013 Whether to show detailed progress during embedding. num_workers ( int , default: None ) \u2013 Number of worker processes to use; defaults to the instance's configuration if not provided. batch_size ( int , default: 1000 ) \u2013 Number of files to process in each batch. Note Uses multiprocessing to enhance performance. Source code in src/embed.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 def embed_and_insert_dataset ( self , input_dir : str , detailed_progress : bool = False , num_workers : Optional [ int ] = None , batch_size : int = 1000 , ) -> None : \"\"\" Processes, embeds, and writes documents from the specified directory to the vectorstore in batches. Args: input_dir (str): Directory containing documents to embed. detailed_progress (bool): Whether to show detailed progress during embedding. num_workers (int, optional): Number of worker processes to use; defaults to the instance's configuration if not provided. batch_size (int): Number of files to process in each batch. Note: Uses multiprocessing to enhance performance. \"\"\" num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( input_dir ))) with tqdm ( total = num_files , desc = \"Embedding files\" , unit = \" files\" , smoothing = 0 ) as pbar : while True : file_chunk = list ( islice ( get_files_from_dir ( input_dir ), batch_size ) ) if not file_chunk : break self . embed_and_insert_files ( file_chunk ) pbar . update ( len ( file_chunk ))","title":"embed_and_insert_dataset"},{"location":"api-reference/embedder/#src.embed.Embedder.embed_and_insert_docs","text":"Embeds documents and inserts their embeddings into the vectorstore, then returns the IDs, documents, and embeddings. Parameters: docs ( List [ EnhancedDocument ] ) \u2013 Documents to embed and insert. Returns: tuple ( Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]] ) \u2013 A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError \u2013 If the vectorstore instance is not set. Source code in src/embed.py 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def embed_and_insert_docs ( self , docs : List [ EnhancedDocument ] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Embeds documents and inserts their embeddings into the vectorstore, then returns the IDs, documents, and embeddings. Args: docs (List[EnhancedDocument]): Documents to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError: If the vectorstore instance is not set. \"\"\" self . _verify_vectorstore_client () embeddings = self . embed_docs ( docs ) ids , docs , embeddings = self . insert_embeddings ( docs , embeddings ) return ids , docs , embeddings","title":"embed_and_insert_docs"},{"location":"api-reference/embedder/#src.embed.Embedder.embed_and_insert_files","text":"Embeds documents from specified file paths and inserts them into the vector store. Parameters: file_paths ( List [ str ] ) \u2013 File paths to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Source code in src/embed.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def embed_and_insert_files ( self , file_paths : List [ str ] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Embeds documents from specified file paths and inserts them into the vector store. Args: file_paths (List[str]): File paths to embed and insert. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. \"\"\" self . _verify_vectorstore_client () all_docs = [] all_embeddings = [] for file in file_paths : curr_docs , curr_embeddings = self . embed_files ( file_paths ) # NOTE(STP): We're not calling self.embed_and_insert_docs() here # in order to allow us to batch embed multiple files. all_docs . extend ( curr_docs ) all_embeddings . extend ( curr_embeddings ) docs , ids , embeddings = self . insert_embeddings ( all_docs , all_embeddings ) return ids , docs , embeddings","title":"embed_and_insert_files"},{"location":"api-reference/embedder/#src.embed.Embedder.embed_docs","text":"Generates embeddings for a list of documents. Parameters: docs ( List [ EnhancedDocument ] ) \u2013 Documents to embed. Returns: List [ List [ float ]] \u2013 List[List[float]]: List of embeddings for each document. Source code in src/embed.py 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 def embed_docs ( self , docs : List [ EnhancedDocument ]) -> List [ List [ float ]]: \"\"\" Generates embeddings for a list of documents. Args: docs (List[EnhancedDocument]): Documents to embed. Returns: List[List[float]]: List of embeddings for each document. \"\"\" # NOTE(STP): This ignores metadata. If we want to include metadata in # the embedding, we would need to combine it with the page content # and stringify it in some manner. # TODO(STP): We might want to batch embed documents here if the number # of documents exceed a certain threshold. Would need to look more into # if and when that would be useful. logging . debug ( \"Embedding %d docs\" , len ( docs )) page_contents = [ doc . page_content for doc in docs ] embeddings = self . embedder . embed_documents ( page_contents ) logging . debug ( \"Embedded %d docs\" , len ( docs )) return embeddings","title":"embed_docs"},{"location":"api-reference/embedder/#src.embed.Embedder.embed_files","text":"Embeds a batch of files specified by their paths. Parameters: file_paths ( List [ str ] ) \u2013 List of file paths to embed. Returns: tuple ( Tuple [ EnhancedDocument , List [ List [ float ]]] ) \u2013 A tuple containing lists of ids, docs, and their embeddings. Source code in src/embed.py 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def embed_files ( self , file_paths : List [ str ] ) -> Tuple [ EnhancedDocument , List [ List [ float ]]]: \"\"\" Embeds a batch of files specified by their paths. Args: file_paths (List[str]): List of file paths to embed. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. \"\"\" # NOTE(STP): We allow passing multiple files to take advantage of # batching benefits. logging . debug ( \"Embedding files: %s \" , file_paths ) docs = [] for file_path in file_paths : docs . extend ( load_docs_from_jsonl ( file_path )) embeddings = self . embed_docs ( docs ) logging . debug ( \"Embedded files: %s \" , file_paths ) return docs , embeddings","title":"embed_files"},{"location":"api-reference/embedder/#src.embed.Embedder.insert_embeddings","text":"Inserts the embeddings of the provided documents into the vectorstore and ensures all documents are unique based on their content hash. Parameters: docs ( List [ EnhancedDocument ] ) \u2013 Documents whose embeddings are to be inserted. embeddings ( List [ List [ float ]] ) \u2013 Embeddings corresponding to the documents. Returns: tuple ( Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]] ) \u2013 A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError \u2013 If the vectorstore instance is not set. Source code in src/embed.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 def insert_embeddings ( self , docs : List [ EnhancedDocument ], embeddings : List [ List [ float ]] ) -> Tuple [ List [ str ], List [ EnhancedDocument ], List [ List [ float ]]]: \"\"\" Inserts the embeddings of the provided documents into the vectorstore and ensures all documents are unique based on their content hash. Args: docs (List[EnhancedDocument]): Documents whose embeddings are to be inserted. embeddings (List[List[float]]): Embeddings corresponding to the documents. Returns: tuple: A tuple containing lists of ids, docs, and their embeddings. Raises: ValueError: If the vectorstore instance is not set. \"\"\" self . _verify_vectorstore_client () logging . debug ( \"Saving %d embedded docs to vectorstore docs\" , len ( docs )) ids = [ doc . document_hash for doc in docs ] if len ( ids ) != len ( set ( ids )): # TODO(STP): Improve space efficiency here. unique_docs = [] unique_embeddings = [] unique_ids = [] seen = set () for i , curr_id in enumerate ( ids ): if curr_id in seen : logging . debug ( \"Found multiple documents from %s with the \" \" same content hash. ' %s ...'\" , docs [ i ] . metadata [ \"source\" ], docs [ i ] . page_content [: 30 ], ) else : unique_ids . append ( curr_id ) unique_docs . append ( docs [ i ]) unique_embeddings . append ( embeddings [ i ]) seen . add ( curr_id ) docs = unique_docs ids = unique_ids embeddings = unique_embeddings texts = [ doc . page_content for doc in docs ] text_embeddings = zip ( texts , embeddings ) metadatas = [ doc . metadata for doc in docs ] self . vectorstore_instance . add_embeddings ( text_embeddings = text_embeddings , ids = ids , metadatas = metadatas ) logging . debug ( \"Saved %d embedded docs to vectorstore docs\" , len ( docs )) return ids , docs , embeddings","title":"insert_embeddings"},{"location":"api-reference/embedder/#src.embed.Embedder.save_vectorstore","text":"Saves the current state of the vector store locally. Source code in src/embed.py 274 275 276 277 278 279 280 281 282 283 284 285 286 def save_vectorstore ( self ) -> None : \"\"\" Saves the current state of the vector store locally. \"\"\" if self . vectorstore_name == \"FAISS\" : save_local_config = self . vectorstore_config [ \"FAISS\" ][ \"save_local_config\" ] if save_local_config [ \"save_local\" ]: self . vectorstore_client . save_local ( save_local_config [ \"folder_path\" ], save_local_config [ \"index_name\" ], )","title":"save_vectorstore"},{"location":"api-reference/embedder/#src.embed.Embedder.set_embedder","text":"Configures and initializes the embedder based on specified name and configuration. Parameters: name ( str ) \u2013 Name of the embedder to configure. config ( dict ) \u2013 Configuration dictionary for the embedder. Raises: NotImplementedError \u2013 If a 'custom' embedder is specified but not implemented. ValueError \u2013 If embedder name is not recognized or none provided when required. Source code in src/embed.py 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 def set_embedder ( self , name : str , config : Dict ) -> Embeddings : \"\"\" Configures and initializes the embedder based on specified name and configuration. Args: name (str): Name of the embedder to configure. config (dict): Configuration dictionary for the embedder. Raises: NotImplementedError: If a 'custom' embedder is specified but not implemented. ValueError: If embedder name is not recognized or none provided when required. \"\"\" if name == \"custom\" : error_message = \"\"\" \"If using custom embedder, the Embedder.set_embedder() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) embedder_config = config [ name ] if name == \"OpenAI\" : return OpenAIEmbeddings ( ** embedder_config ) elif name == \"HuggingFace\" : return HuggingFaceEmbeddings ( ** embedder_config ) else : raise ValueError ( \"Embedding not recognized: %s \" , name )","title":"set_embedder"},{"location":"api-reference/embedder/#src.embed.Embedder.set_vectorstore","text":"Configures and initializes the vector store based on specified name and configuration. Parameters: name ( str ) \u2013 Name of the vector store to configure. config ( dict ) \u2013 Configuration dictionary for the vector store. Raises: NotImplementedError \u2013 If a 'custom' vector store is specified but not implemented. ValueError \u2013 If vector store name is not recognized or none provided when required. Source code in src/embed.py 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 def set_vectorstore ( self , name : str , config : Dict ): \"\"\" Configures and initializes the vector store based on specified name and configuration. Args: name (str): Name of the vector store to configure. config (dict): Configuration dictionary for the vector store. Raises: NotImplementedError: If a 'custom' vector store is specified but not implemented. ValueError: If vector store name is not recognized or none provided when required. \"\"\" assert name is not None and name in self . ALLOWED_VECTORSTORES if name == \"custom\" : error_message = \"\"\" \"If using custom vectorstore, the Embedder.set_vectorstore() method must be overridden. \"\"\" raise NotImplementedError ( error_message ) self . vectorstore_name = name self . vectorstore_config = config config = config [ name ] if name == \"FAISS\" : if config [ \"load_local\" ]: load_local_config = config [ \"load_local_args\" ] load_local_config [ \"embeddings\" ] = self . embedder vectorstore_instance = FAISS . load_local ( ** load_local_config ) num_documents = len ( vectorstore_instance . index_to_docstore_id ) logging . debug ( \"Total number of documents loaded from saved FAISS \" \"vectorstore: %d \" , num_documents , ) else : config = config [ \"init_args\" ] config [ \"embedding_function\" ] = self . embedder config [ \"index\" ] = faiss . IndexFlatL2 ( self . embedder . client . get_sentence_embedding_dimension () ) config [ \"docstore\" ] = InMemoryDocstore () config [ \"index_to_docstore_id\" ] = {} vectorstore_instance = FAISS ( ** config ) self . vectorstore_instance = vectorstore_instance","title":"set_vectorstore"},{"location":"api-reference/enhanced_document/","text":"EnhancedDocument API Reference \u00b6 Module contains logic for indexing documents into vector stores. EnhancedDocument \u00b6 Bases: Document A hashed document with a unique ID. Source code in src/enhanced_document.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class EnhancedDocument ( Document ): \"\"\"A hashed document with a unique ID.\"\"\" source : str \"\"\"The file path of the document.\"\"\" document_hash : str \"\"\"The hash of the document including content and metadata.\"\"\" content_hash : str \"\"\"The hash of the document content.\"\"\" metadata_hash : str \"\"\"The hash of the document metadata.\"\"\" @root_validator ( pre = True ) def calculate_hashes_and_source ( cls , values ) -> Dict [ str , Any ]: \"\"\"Calculate content, metadata and overall document hash. Also, update the metadata to include these hashes in there, in order to make it easier to query on them if required. \"\"\" content = values . get ( \"page_content\" ) metadata = values . get ( \"metadata\" ) if \"source\" not in metadata : raise KeyError ( \"'source' not found in metadata. Each EnhancedDocument must \" \"have a source.\" ) values [ \"source\" ] = metadata [ \"source\" ] forbidden_keys = ( \"document_hash\" , \"content_hash\" , \"metadata_hash\" ) # HACK(STP): If we're reloading EnhancedDocuments from their JSON # representation, the forbidden keys will already be present. We # simply use them here. if all ( key in metadata for key in forbidden_keys ): for key in forbidden_keys : values [ key ] = metadata [ key ] else : for key in forbidden_keys : if key in metadata : raise ValueError ( f \"Metadata cannot contain key { key } as it \" f \"is reserved for internal use.\" ) content_hash = str ( _hash_string_to_uuid ( content )) try : metadata_hash = str ( _hash_nested_dict_to_uuid ( metadata )) except Exception as e : raise ValueError ( f \"Failed to hash metadata: { e } . \" f \"Please use a dict that can be serialized using json.\" ) document_hash = str ( _hash_string_to_uuid ( content_hash + metadata_hash ) ) # Update metadata with hashes hashes = {} hashes [ \"content_hash\" ] = content_hash hashes [ \"metadata_hash\" ] = metadata_hash hashes [ \"document_hash\" ] = document_hash metadata . update ( hashes ) # Set hash values in the model # Ensure values are explicitly set values [ \"content_hash\" ] = content_hash values [ \"metadata_hash\" ] = metadata_hash values [ \"document_hash\" ] = document_hash return values def to_document ( self ) -> Document : \"\"\"Return a Document object.\"\"\" return Document ( page_content = self . page_content , metadata = self . metadata , ) @classmethod def from_document ( cls , document : Document , * , uid : Optional [ str ] = None ) -> EnhancedDocument : \"\"\"Create a HashedDocument from a Document.\"\"\" return cls ( # type: ignore[call-arg] uid = uid , # type: ignore[arg-type] page_content = document . page_content , metadata = document . metadata , ) @classmethod def remove_hashes ( cls , document : Document ) -> Document : forbidden_keys = ( \"document_hash\" , \"content_hash\" , \"metadata_hash\" ) metadata = document . metadata for key in forbidden_keys : if key in metadata : del metadata [ key ] return document content_hash : str instance-attribute \u00b6 The hash of the document content. document_hash : str instance-attribute \u00b6 The hash of the document including content and metadata. metadata_hash : str instance-attribute \u00b6 The hash of the document metadata. source : str instance-attribute \u00b6 The file path of the document. calculate_hashes_and_source ( values ) \u00b6 Calculate content, metadata and overall document hash. Also, update the metadata to include these hashes in there, in order to make it easier to query on them if required. Source code in src/enhanced_document.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 @root_validator ( pre = True ) def calculate_hashes_and_source ( cls , values ) -> Dict [ str , Any ]: \"\"\"Calculate content, metadata and overall document hash. Also, update the metadata to include these hashes in there, in order to make it easier to query on them if required. \"\"\" content = values . get ( \"page_content\" ) metadata = values . get ( \"metadata\" ) if \"source\" not in metadata : raise KeyError ( \"'source' not found in metadata. Each EnhancedDocument must \" \"have a source.\" ) values [ \"source\" ] = metadata [ \"source\" ] forbidden_keys = ( \"document_hash\" , \"content_hash\" , \"metadata_hash\" ) # HACK(STP): If we're reloading EnhancedDocuments from their JSON # representation, the forbidden keys will already be present. We # simply use them here. if all ( key in metadata for key in forbidden_keys ): for key in forbidden_keys : values [ key ] = metadata [ key ] else : for key in forbidden_keys : if key in metadata : raise ValueError ( f \"Metadata cannot contain key { key } as it \" f \"is reserved for internal use.\" ) content_hash = str ( _hash_string_to_uuid ( content )) try : metadata_hash = str ( _hash_nested_dict_to_uuid ( metadata )) except Exception as e : raise ValueError ( f \"Failed to hash metadata: { e } . \" f \"Please use a dict that can be serialized using json.\" ) document_hash = str ( _hash_string_to_uuid ( content_hash + metadata_hash ) ) # Update metadata with hashes hashes = {} hashes [ \"content_hash\" ] = content_hash hashes [ \"metadata_hash\" ] = metadata_hash hashes [ \"document_hash\" ] = document_hash metadata . update ( hashes ) # Set hash values in the model # Ensure values are explicitly set values [ \"content_hash\" ] = content_hash values [ \"metadata_hash\" ] = metadata_hash values [ \"document_hash\" ] = document_hash return values from_document ( document , * , uid = None ) classmethod \u00b6 Create a HashedDocument from a Document. Source code in src/enhanced_document.py 115 116 117 118 119 120 121 122 123 124 @classmethod def from_document ( cls , document : Document , * , uid : Optional [ str ] = None ) -> EnhancedDocument : \"\"\"Create a HashedDocument from a Document.\"\"\" return cls ( # type: ignore[call-arg] uid = uid , # type: ignore[arg-type] page_content = document . page_content , metadata = document . metadata , ) to_document () \u00b6 Return a Document object. Source code in src/enhanced_document.py 108 109 110 111 112 113 def to_document ( self ) -> Document : \"\"\"Return a Document object.\"\"\" return Document ( page_content = self . page_content , metadata = self . metadata , )","title":"EnhancedDocument"},{"location":"api-reference/enhanced_document/#enhanceddocument-api-reference","text":"Module contains logic for indexing documents into vector stores.","title":"EnhancedDocument API Reference"},{"location":"api-reference/enhanced_document/#src.enhanced_document.EnhancedDocument","text":"Bases: Document A hashed document with a unique ID. Source code in src/enhanced_document.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 class EnhancedDocument ( Document ): \"\"\"A hashed document with a unique ID.\"\"\" source : str \"\"\"The file path of the document.\"\"\" document_hash : str \"\"\"The hash of the document including content and metadata.\"\"\" content_hash : str \"\"\"The hash of the document content.\"\"\" metadata_hash : str \"\"\"The hash of the document metadata.\"\"\" @root_validator ( pre = True ) def calculate_hashes_and_source ( cls , values ) -> Dict [ str , Any ]: \"\"\"Calculate content, metadata and overall document hash. Also, update the metadata to include these hashes in there, in order to make it easier to query on them if required. \"\"\" content = values . get ( \"page_content\" ) metadata = values . get ( \"metadata\" ) if \"source\" not in metadata : raise KeyError ( \"'source' not found in metadata. Each EnhancedDocument must \" \"have a source.\" ) values [ \"source\" ] = metadata [ \"source\" ] forbidden_keys = ( \"document_hash\" , \"content_hash\" , \"metadata_hash\" ) # HACK(STP): If we're reloading EnhancedDocuments from their JSON # representation, the forbidden keys will already be present. We # simply use them here. if all ( key in metadata for key in forbidden_keys ): for key in forbidden_keys : values [ key ] = metadata [ key ] else : for key in forbidden_keys : if key in metadata : raise ValueError ( f \"Metadata cannot contain key { key } as it \" f \"is reserved for internal use.\" ) content_hash = str ( _hash_string_to_uuid ( content )) try : metadata_hash = str ( _hash_nested_dict_to_uuid ( metadata )) except Exception as e : raise ValueError ( f \"Failed to hash metadata: { e } . \" f \"Please use a dict that can be serialized using json.\" ) document_hash = str ( _hash_string_to_uuid ( content_hash + metadata_hash ) ) # Update metadata with hashes hashes = {} hashes [ \"content_hash\" ] = content_hash hashes [ \"metadata_hash\" ] = metadata_hash hashes [ \"document_hash\" ] = document_hash metadata . update ( hashes ) # Set hash values in the model # Ensure values are explicitly set values [ \"content_hash\" ] = content_hash values [ \"metadata_hash\" ] = metadata_hash values [ \"document_hash\" ] = document_hash return values def to_document ( self ) -> Document : \"\"\"Return a Document object.\"\"\" return Document ( page_content = self . page_content , metadata = self . metadata , ) @classmethod def from_document ( cls , document : Document , * , uid : Optional [ str ] = None ) -> EnhancedDocument : \"\"\"Create a HashedDocument from a Document.\"\"\" return cls ( # type: ignore[call-arg] uid = uid , # type: ignore[arg-type] page_content = document . page_content , metadata = document . metadata , ) @classmethod def remove_hashes ( cls , document : Document ) -> Document : forbidden_keys = ( \"document_hash\" , \"content_hash\" , \"metadata_hash\" ) metadata = document . metadata for key in forbidden_keys : if key in metadata : del metadata [ key ] return document","title":"EnhancedDocument"},{"location":"api-reference/enhanced_document/#src.enhanced_document.EnhancedDocument.content_hash","text":"The hash of the document content.","title":"content_hash"},{"location":"api-reference/enhanced_document/#src.enhanced_document.EnhancedDocument.document_hash","text":"The hash of the document including content and metadata.","title":"document_hash"},{"location":"api-reference/enhanced_document/#src.enhanced_document.EnhancedDocument.metadata_hash","text":"The hash of the document metadata.","title":"metadata_hash"},{"location":"api-reference/enhanced_document/#src.enhanced_document.EnhancedDocument.source","text":"The file path of the document.","title":"source"},{"location":"api-reference/enhanced_document/#src.enhanced_document.EnhancedDocument.calculate_hashes_and_source","text":"Calculate content, metadata and overall document hash. Also, update the metadata to include these hashes in there, in order to make it easier to query on them if required. Source code in src/enhanced_document.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 @root_validator ( pre = True ) def calculate_hashes_and_source ( cls , values ) -> Dict [ str , Any ]: \"\"\"Calculate content, metadata and overall document hash. Also, update the metadata to include these hashes in there, in order to make it easier to query on them if required. \"\"\" content = values . get ( \"page_content\" ) metadata = values . get ( \"metadata\" ) if \"source\" not in metadata : raise KeyError ( \"'source' not found in metadata. Each EnhancedDocument must \" \"have a source.\" ) values [ \"source\" ] = metadata [ \"source\" ] forbidden_keys = ( \"document_hash\" , \"content_hash\" , \"metadata_hash\" ) # HACK(STP): If we're reloading EnhancedDocuments from their JSON # representation, the forbidden keys will already be present. We # simply use them here. if all ( key in metadata for key in forbidden_keys ): for key in forbidden_keys : values [ key ] = metadata [ key ] else : for key in forbidden_keys : if key in metadata : raise ValueError ( f \"Metadata cannot contain key { key } as it \" f \"is reserved for internal use.\" ) content_hash = str ( _hash_string_to_uuid ( content )) try : metadata_hash = str ( _hash_nested_dict_to_uuid ( metadata )) except Exception as e : raise ValueError ( f \"Failed to hash metadata: { e } . \" f \"Please use a dict that can be serialized using json.\" ) document_hash = str ( _hash_string_to_uuid ( content_hash + metadata_hash ) ) # Update metadata with hashes hashes = {} hashes [ \"content_hash\" ] = content_hash hashes [ \"metadata_hash\" ] = metadata_hash hashes [ \"document_hash\" ] = document_hash metadata . update ( hashes ) # Set hash values in the model # Ensure values are explicitly set values [ \"content_hash\" ] = content_hash values [ \"metadata_hash\" ] = metadata_hash values [ \"document_hash\" ] = document_hash return values","title":"calculate_hashes_and_source"},{"location":"api-reference/enhanced_document/#src.enhanced_document.EnhancedDocument.from_document","text":"Create a HashedDocument from a Document. Source code in src/enhanced_document.py 115 116 117 118 119 120 121 122 123 124 @classmethod def from_document ( cls , document : Document , * , uid : Optional [ str ] = None ) -> EnhancedDocument : \"\"\"Create a HashedDocument from a Document.\"\"\" return cls ( # type: ignore[call-arg] uid = uid , # type: ignore[arg-type] page_content = document . page_content , metadata = document . metadata , )","title":"from_document"},{"location":"api-reference/enhanced_document/#src.enhanced_document.EnhancedDocument.to_document","text":"Return a Document object. Source code in src/enhanced_document.py 108 109 110 111 112 113 def to_document ( self ) -> Document : \"\"\"Return a Document object.\"\"\" return Document ( page_content = self . page_content , metadata = self . metadata , )","title":"to_document"},{"location":"api-reference/ingest/","text":"Ingester API Reference \u00b6 Ingester \u00b6 Ingest files into a vectorstore. Source code in src/ingest.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Ingester : \"\"\"Ingest files into a vectorstore.\"\"\" def __init__ ( self , loader : Loader = Loader (), chunker : Chunker = Chunker (), embedder : Embedder = Embedder (), ) -> None : \"\"\" Initializes an Ingester instance with components for loading, chunking, and embedding documents. Args: loader (Loader): An instance of Loader to handle document loading. chunker (Chunker): An instance of Chunker to handle document chunking. embedder (Embedder): An instance of Embedder to handle document embedding. \"\"\" self . loader : Loader = loader self . chunker : Chunker = chunker self . embedder : Embedder = embedder def ingest_dataset ( self , input_dir : str , is_zipped : bool = False , unzip_dir : str = \"unzipped\" , save_intermediate_docs : bool = False , output_dir : Optional [ str ] = None , num_workers : int = 10 , max_files : Optional [ int ] = None , detailed_progress : bool = False , batch_size : int = 100 , # in number of files ) -> None : \"\"\" Processes a dataset through specified stages: loading, chunking, and embedding. Args: input_dir (str): Directory containing the documents. is_zipped (bool): Whether the input directory is zipped. unzip_dir (str): Directory to unzip files if zipped. save_intermediate_docs (bool): Whether to save the loaded and chunked documents to disk. output_dir (Optional[str]): Directory where processed documents are saved. num_workers (int): Number of worker processes to use. max_files (Optional[int]): Max number of files to process. detailed_progress (bool): Whether to show detailed progress. batch_size (int): Number of files to process in each batch. \"\"\" if is_zipped : directory = self . loader . unzip_dataset ( input_dir , unzip_dir ) else : directory = input_dir num_files = None if detailed_progress : num_files = ( max_files or 612484 or len ( list ( get_files_from_dir ( directory ))) ) with tqdm ( total = num_files , desc = \"Ingesting files\" , unit = \"files\" , smoothing = 0 ) as pbar : batched_docs = [] prev_counter = 0 for i , file_path in enumerate ( get_files_from_dir ( directory )): docs = self . load_and_chunk_file ( save_intermediate_docs = save_intermediate_docs , output_dir = output_dir , file_path = file_path , ) batched_docs . extend ( docs ) if i - prev_counter >= batch_size : self . embedder . embed_and_insert_docs ( batched_docs ) batched_docs = [] pbar . update ( i - prev_counter ) prev_counter = i if max_files is not None and i >= max_files : break if batched_docs : self . embedder . embed_and_insert_docs ( batched_docs ) def load_and_chunk_file ( self , save_intermediate_docs : bool , output_dir : Optional [ str ], file_path : str , ) -> List [ EnhancedDocument ]: \"\"\" Loads and chunks a file, optionally saving both raw and chunked documents. Args: save_intermediate_docs (bool): Whether to save the documents after processing. output_dir (Optional[str]): Directory to save the documents if `save_docs` is True. file_path (str): Path to the file to be processed. Returns: List[EnhancedDocument]: A list of chunked EnhancedDocument objects. Raises: AssertionError: If `save_docs` is True but no output directory is provided. \"\"\" logging . debug ( \"Loading and chunking: %s \" , file_path ) raw_docs = self . loader . file_to_docs ( file_path ) chunked_docs = self . chunker . chunk_docs ( raw_docs ) if save_intermediate_docs : assert output_dir is not None raw_documents_dir = os . path . join ( output_dir , \"raw_documents\" ) chunked_documents_dir = os . path . join ( output_dir , \"chunked_documents\" ) save_docs_to_file ( raw_docs , file_path , raw_documents_dir ) save_docs_to_file ( chunked_docs , file_path , chunked_documents_dir ) logging . debug ( \"Loaded and chunked: %s \" , file_path ) return chunked_docs __init__ ( loader = Loader (), chunker = Chunker (), embedder = Embedder ()) \u00b6 Initializes an Ingester instance with components for loading, chunking, and embedding documents. Parameters: loader ( Loader , default: Loader () ) \u2013 An instance of Loader to handle document loading. chunker ( Chunker , default: Chunker () ) \u2013 An instance of Chunker to handle document chunking. embedder ( Embedder , default: Embedder () ) \u2013 An instance of Embedder to handle document embedding. Source code in src/ingest.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , loader : Loader = Loader (), chunker : Chunker = Chunker (), embedder : Embedder = Embedder (), ) -> None : \"\"\" Initializes an Ingester instance with components for loading, chunking, and embedding documents. Args: loader (Loader): An instance of Loader to handle document loading. chunker (Chunker): An instance of Chunker to handle document chunking. embedder (Embedder): An instance of Embedder to handle document embedding. \"\"\" self . loader : Loader = loader self . chunker : Chunker = chunker self . embedder : Embedder = embedder ingest_dataset ( input_dir , is_zipped = False , unzip_dir = 'unzipped' , save_intermediate_docs = False , output_dir = None , num_workers = 10 , max_files = None , detailed_progress = False , batch_size = 100 ) \u00b6 Processes a dataset through specified stages: loading, chunking, and embedding. Parameters: input_dir ( str ) \u2013 Directory containing the documents. is_zipped ( bool , default: False ) \u2013 Whether the input directory is zipped. unzip_dir ( str , default: 'unzipped' ) \u2013 Directory to unzip files if zipped. save_intermediate_docs ( bool , default: False ) \u2013 Whether to save the loaded and chunked documents to disk. output_dir ( Optional [ str ] , default: None ) \u2013 Directory where processed documents are saved. num_workers ( int , default: 10 ) \u2013 Number of worker processes to use. max_files ( Optional [ int ] , default: None ) \u2013 Max number of files to process. detailed_progress ( bool , default: False ) \u2013 Whether to show detailed progress. batch_size ( int , default: 100 ) \u2013 Number of files to process in each batch. Source code in src/ingest.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def ingest_dataset ( self , input_dir : str , is_zipped : bool = False , unzip_dir : str = \"unzipped\" , save_intermediate_docs : bool = False , output_dir : Optional [ str ] = None , num_workers : int = 10 , max_files : Optional [ int ] = None , detailed_progress : bool = False , batch_size : int = 100 , # in number of files ) -> None : \"\"\" Processes a dataset through specified stages: loading, chunking, and embedding. Args: input_dir (str): Directory containing the documents. is_zipped (bool): Whether the input directory is zipped. unzip_dir (str): Directory to unzip files if zipped. save_intermediate_docs (bool): Whether to save the loaded and chunked documents to disk. output_dir (Optional[str]): Directory where processed documents are saved. num_workers (int): Number of worker processes to use. max_files (Optional[int]): Max number of files to process. detailed_progress (bool): Whether to show detailed progress. batch_size (int): Number of files to process in each batch. \"\"\" if is_zipped : directory = self . loader . unzip_dataset ( input_dir , unzip_dir ) else : directory = input_dir num_files = None if detailed_progress : num_files = ( max_files or 612484 or len ( list ( get_files_from_dir ( directory ))) ) with tqdm ( total = num_files , desc = \"Ingesting files\" , unit = \"files\" , smoothing = 0 ) as pbar : batched_docs = [] prev_counter = 0 for i , file_path in enumerate ( get_files_from_dir ( directory )): docs = self . load_and_chunk_file ( save_intermediate_docs = save_intermediate_docs , output_dir = output_dir , file_path = file_path , ) batched_docs . extend ( docs ) if i - prev_counter >= batch_size : self . embedder . embed_and_insert_docs ( batched_docs ) batched_docs = [] pbar . update ( i - prev_counter ) prev_counter = i if max_files is not None and i >= max_files : break if batched_docs : self . embedder . embed_and_insert_docs ( batched_docs ) load_and_chunk_file ( save_intermediate_docs , output_dir , file_path ) \u00b6 Loads and chunks a file, optionally saving both raw and chunked documents. Parameters: save_intermediate_docs ( bool ) \u2013 Whether to save the documents after processing. output_dir ( Optional [ str ] ) \u2013 Directory to save the documents if save_docs is True. file_path ( str ) \u2013 Path to the file to be processed. Returns: List [ EnhancedDocument ] \u2013 List[EnhancedDocument]: A list of chunked EnhancedDocument objects. Raises: AssertionError \u2013 If save_docs is True but no output directory is provided. Source code in src/ingest.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def load_and_chunk_file ( self , save_intermediate_docs : bool , output_dir : Optional [ str ], file_path : str , ) -> List [ EnhancedDocument ]: \"\"\" Loads and chunks a file, optionally saving both raw and chunked documents. Args: save_intermediate_docs (bool): Whether to save the documents after processing. output_dir (Optional[str]): Directory to save the documents if `save_docs` is True. file_path (str): Path to the file to be processed. Returns: List[EnhancedDocument]: A list of chunked EnhancedDocument objects. Raises: AssertionError: If `save_docs` is True but no output directory is provided. \"\"\" logging . debug ( \"Loading and chunking: %s \" , file_path ) raw_docs = self . loader . file_to_docs ( file_path ) chunked_docs = self . chunker . chunk_docs ( raw_docs ) if save_intermediate_docs : assert output_dir is not None raw_documents_dir = os . path . join ( output_dir , \"raw_documents\" ) chunked_documents_dir = os . path . join ( output_dir , \"chunked_documents\" ) save_docs_to_file ( raw_docs , file_path , raw_documents_dir ) save_docs_to_file ( chunked_docs , file_path , chunked_documents_dir ) logging . debug ( \"Loaded and chunked: %s \" , file_path ) return chunked_docs","title":"Ingester"},{"location":"api-reference/ingest/#ingester-api-reference","text":"","title":"Ingester API Reference"},{"location":"api-reference/ingest/#src.ingest.Ingester","text":"Ingest files into a vectorstore. Source code in src/ingest.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 class Ingester : \"\"\"Ingest files into a vectorstore.\"\"\" def __init__ ( self , loader : Loader = Loader (), chunker : Chunker = Chunker (), embedder : Embedder = Embedder (), ) -> None : \"\"\" Initializes an Ingester instance with components for loading, chunking, and embedding documents. Args: loader (Loader): An instance of Loader to handle document loading. chunker (Chunker): An instance of Chunker to handle document chunking. embedder (Embedder): An instance of Embedder to handle document embedding. \"\"\" self . loader : Loader = loader self . chunker : Chunker = chunker self . embedder : Embedder = embedder def ingest_dataset ( self , input_dir : str , is_zipped : bool = False , unzip_dir : str = \"unzipped\" , save_intermediate_docs : bool = False , output_dir : Optional [ str ] = None , num_workers : int = 10 , max_files : Optional [ int ] = None , detailed_progress : bool = False , batch_size : int = 100 , # in number of files ) -> None : \"\"\" Processes a dataset through specified stages: loading, chunking, and embedding. Args: input_dir (str): Directory containing the documents. is_zipped (bool): Whether the input directory is zipped. unzip_dir (str): Directory to unzip files if zipped. save_intermediate_docs (bool): Whether to save the loaded and chunked documents to disk. output_dir (Optional[str]): Directory where processed documents are saved. num_workers (int): Number of worker processes to use. max_files (Optional[int]): Max number of files to process. detailed_progress (bool): Whether to show detailed progress. batch_size (int): Number of files to process in each batch. \"\"\" if is_zipped : directory = self . loader . unzip_dataset ( input_dir , unzip_dir ) else : directory = input_dir num_files = None if detailed_progress : num_files = ( max_files or 612484 or len ( list ( get_files_from_dir ( directory ))) ) with tqdm ( total = num_files , desc = \"Ingesting files\" , unit = \"files\" , smoothing = 0 ) as pbar : batched_docs = [] prev_counter = 0 for i , file_path in enumerate ( get_files_from_dir ( directory )): docs = self . load_and_chunk_file ( save_intermediate_docs = save_intermediate_docs , output_dir = output_dir , file_path = file_path , ) batched_docs . extend ( docs ) if i - prev_counter >= batch_size : self . embedder . embed_and_insert_docs ( batched_docs ) batched_docs = [] pbar . update ( i - prev_counter ) prev_counter = i if max_files is not None and i >= max_files : break if batched_docs : self . embedder . embed_and_insert_docs ( batched_docs ) def load_and_chunk_file ( self , save_intermediate_docs : bool , output_dir : Optional [ str ], file_path : str , ) -> List [ EnhancedDocument ]: \"\"\" Loads and chunks a file, optionally saving both raw and chunked documents. Args: save_intermediate_docs (bool): Whether to save the documents after processing. output_dir (Optional[str]): Directory to save the documents if `save_docs` is True. file_path (str): Path to the file to be processed. Returns: List[EnhancedDocument]: A list of chunked EnhancedDocument objects. Raises: AssertionError: If `save_docs` is True but no output directory is provided. \"\"\" logging . debug ( \"Loading and chunking: %s \" , file_path ) raw_docs = self . loader . file_to_docs ( file_path ) chunked_docs = self . chunker . chunk_docs ( raw_docs ) if save_intermediate_docs : assert output_dir is not None raw_documents_dir = os . path . join ( output_dir , \"raw_documents\" ) chunked_documents_dir = os . path . join ( output_dir , \"chunked_documents\" ) save_docs_to_file ( raw_docs , file_path , raw_documents_dir ) save_docs_to_file ( chunked_docs , file_path , chunked_documents_dir ) logging . debug ( \"Loaded and chunked: %s \" , file_path ) return chunked_docs","title":"Ingester"},{"location":"api-reference/ingest/#src.ingest.Ingester.__init__","text":"Initializes an Ingester instance with components for loading, chunking, and embedding documents. Parameters: loader ( Loader , default: Loader () ) \u2013 An instance of Loader to handle document loading. chunker ( Chunker , default: Chunker () ) \u2013 An instance of Chunker to handle document chunking. embedder ( Embedder , default: Embedder () ) \u2013 An instance of Embedder to handle document embedding. Source code in src/ingest.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 def __init__ ( self , loader : Loader = Loader (), chunker : Chunker = Chunker (), embedder : Embedder = Embedder (), ) -> None : \"\"\" Initializes an Ingester instance with components for loading, chunking, and embedding documents. Args: loader (Loader): An instance of Loader to handle document loading. chunker (Chunker): An instance of Chunker to handle document chunking. embedder (Embedder): An instance of Embedder to handle document embedding. \"\"\" self . loader : Loader = loader self . chunker : Chunker = chunker self . embedder : Embedder = embedder","title":"__init__"},{"location":"api-reference/ingest/#src.ingest.Ingester.ingest_dataset","text":"Processes a dataset through specified stages: loading, chunking, and embedding. Parameters: input_dir ( str ) \u2013 Directory containing the documents. is_zipped ( bool , default: False ) \u2013 Whether the input directory is zipped. unzip_dir ( str , default: 'unzipped' ) \u2013 Directory to unzip files if zipped. save_intermediate_docs ( bool , default: False ) \u2013 Whether to save the loaded and chunked documents to disk. output_dir ( Optional [ str ] , default: None ) \u2013 Directory where processed documents are saved. num_workers ( int , default: 10 ) \u2013 Number of worker processes to use. max_files ( Optional [ int ] , default: None ) \u2013 Max number of files to process. detailed_progress ( bool , default: False ) \u2013 Whether to show detailed progress. batch_size ( int , default: 100 ) \u2013 Number of files to process in each batch. Source code in src/ingest.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 def ingest_dataset ( self , input_dir : str , is_zipped : bool = False , unzip_dir : str = \"unzipped\" , save_intermediate_docs : bool = False , output_dir : Optional [ str ] = None , num_workers : int = 10 , max_files : Optional [ int ] = None , detailed_progress : bool = False , batch_size : int = 100 , # in number of files ) -> None : \"\"\" Processes a dataset through specified stages: loading, chunking, and embedding. Args: input_dir (str): Directory containing the documents. is_zipped (bool): Whether the input directory is zipped. unzip_dir (str): Directory to unzip files if zipped. save_intermediate_docs (bool): Whether to save the loaded and chunked documents to disk. output_dir (Optional[str]): Directory where processed documents are saved. num_workers (int): Number of worker processes to use. max_files (Optional[int]): Max number of files to process. detailed_progress (bool): Whether to show detailed progress. batch_size (int): Number of files to process in each batch. \"\"\" if is_zipped : directory = self . loader . unzip_dataset ( input_dir , unzip_dir ) else : directory = input_dir num_files = None if detailed_progress : num_files = ( max_files or 612484 or len ( list ( get_files_from_dir ( directory ))) ) with tqdm ( total = num_files , desc = \"Ingesting files\" , unit = \"files\" , smoothing = 0 ) as pbar : batched_docs = [] prev_counter = 0 for i , file_path in enumerate ( get_files_from_dir ( directory )): docs = self . load_and_chunk_file ( save_intermediate_docs = save_intermediate_docs , output_dir = output_dir , file_path = file_path , ) batched_docs . extend ( docs ) if i - prev_counter >= batch_size : self . embedder . embed_and_insert_docs ( batched_docs ) batched_docs = [] pbar . update ( i - prev_counter ) prev_counter = i if max_files is not None and i >= max_files : break if batched_docs : self . embedder . embed_and_insert_docs ( batched_docs )","title":"ingest_dataset"},{"location":"api-reference/ingest/#src.ingest.Ingester.load_and_chunk_file","text":"Loads and chunks a file, optionally saving both raw and chunked documents. Parameters: save_intermediate_docs ( bool ) \u2013 Whether to save the documents after processing. output_dir ( Optional [ str ] ) \u2013 Directory to save the documents if save_docs is True. file_path ( str ) \u2013 Path to the file to be processed. Returns: List [ EnhancedDocument ] \u2013 List[EnhancedDocument]: A list of chunked EnhancedDocument objects. Raises: AssertionError \u2013 If save_docs is True but no output directory is provided. Source code in src/ingest.py 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 def load_and_chunk_file ( self , save_intermediate_docs : bool , output_dir : Optional [ str ], file_path : str , ) -> List [ EnhancedDocument ]: \"\"\" Loads and chunks a file, optionally saving both raw and chunked documents. Args: save_intermediate_docs (bool): Whether to save the documents after processing. output_dir (Optional[str]): Directory to save the documents if `save_docs` is True. file_path (str): Path to the file to be processed. Returns: List[EnhancedDocument]: A list of chunked EnhancedDocument objects. Raises: AssertionError: If `save_docs` is True but no output directory is provided. \"\"\" logging . debug ( \"Loading and chunking: %s \" , file_path ) raw_docs = self . loader . file_to_docs ( file_path ) chunked_docs = self . chunker . chunk_docs ( raw_docs ) if save_intermediate_docs : assert output_dir is not None raw_documents_dir = os . path . join ( output_dir , \"raw_documents\" ) chunked_documents_dir = os . path . join ( output_dir , \"chunked_documents\" ) save_docs_to_file ( raw_docs , file_path , raw_documents_dir ) save_docs_to_file ( chunked_docs , file_path , chunked_documents_dir ) logging . debug ( \"Loaded and chunked: %s \" , file_path ) return chunked_docs","title":"load_and_chunk_file"},{"location":"api-reference/load/","text":"Loader API Reference \u00b6 Loader \u00b6 Load files into a standardized format. Potential functions to override if implementing a custom Loader class: - file_to_docs() : the logic for how a file is converted to an EnhancedDocument. Source code in src/load.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 class Loader : \"\"\"Load files into a standardized format. Potential functions to override if implementing a custom Loader class: - `file_to_docs()`: the logic for how a file is converted to an EnhancedDocument. \"\"\" def __init__ ( self , autoloader_config : dict = DEFAULT_AUTOLOADER_CONFIG , ) -> None : \"\"\" Initializes a Loader instance with a given autoloader configuration. Args: autoloader_config (dict): Configuration for autoloaders that determines how different file types are processed. Attributes: autoloader_config (dict): Stores the provided autoloader configuration. autoloaders (Set[str]): Set of valid autoloaders based on the configuration. \"\"\" self . autoloader_config : dict = autoloader_config self . autoloaders : Set [ str ] = self . _get_valid_autoloaders () def load_dataset ( self , input_dir : str , output_dir : str , is_zipped : bool = False , unzip_dir : str = \"unzipped\" , detailed_progress : bool = False , num_workers : int = 10 , max_files : Optional [ int ] = None , ) -> None : \"\"\" Loads a dataset from a specified directory, processes files into EnhancedDocument objects, and saves them to disk. Args: input_dir (str): Path to the directory containing the dataset. is_zipped (bool): Whether the dataset is in a zipped format. unzip_dir (str): Directory to unzip files to, if applicable. output_dir (str): Directory where processed documents should be saved. detailed_progress (bool): Whether to display detailed progress information. num_workers (int): Number of worker processes to use for loading files. max_files (int, optional): Maximum number of files to process. \"\"\" logging . debug ( \"Loading dataset from %s \" , input_dir ) if is_zipped : # TODO(STP): Make this check cleaner. dataset_name = os . path . basename ( input_dir )[: - 4 ] if dataset_name [: - 4 ] != \".zip\" : raise ValueError ( \"Zipped dataset name must end in '.zip'. Received dataset \" \"name: %s \" , dataset_name , ) directory = os . path . join ( unzip_dir , dataset_name ) directory = self . unzip_dataset ( input_dir , unzip_dir ) else : directory = input_dir num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( directory ))) partial_func = partial ( self . load_file , save_docs = True , output_dir = output_dir ) with tqdm ( total = num_files , desc = \"Loading files\" , unit = \"files\" , smoothing = 0 ) as pbar : with multiprocessing . Pool ( num_workers ) as pool : for i , _ in enumerate ( pool . imap_unordered ( partial_func , get_files_from_dir ( directory ), ) ): pbar . update ( 1 ) if max_files is not None and i + 1 >= max_files : break def load_file ( self , save_docs : bool , output_dir : Optional [ str ], file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Loads a single file from the given path and optionally saves the processed document. Args: save_docs (bool): Whether to save the processed documents. output_dir (str, optional): Directory where processed documents should be saved. file_path (str): Path to the file being loaded. Raises: AssertionError: If `save_docs` is True but `output_dir` is None. \"\"\" logging . debug ( \"Loading file: %s \" , file_path ) docs = self . file_to_docs ( file_path ) if save_docs : assert output_dir is not None save_docs_to_file ( docs , file_path , output_dir ) logging . debug ( \"Loaded file: %s \" , file_path ) return docs def file_to_docs ( self , file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Processes a file into a list of EnhancedDocument objects based on the file extension and configured autoloaders. Args: file_path (str): Path to the file being processed. Returns: List[EnhancedDocument]: A list of EnhancedDocument objects created from the file. \"\"\" # NOTE(STP): Switching to unstructured's file-type detection in the # future might be worthwhile (although their check for whether a file # is a JSON file is whether or not json.load() succeeds, which might # not be performant?). # See https://github.com/Unstructured-IO/unstructured/blob/main/unstructured/file_utils/filetype.py # noqa: E501 file_extension = file_path . split ( \".\" )[ - 1 ] if file_extension == \"json\" and \"JSONLoader\" in self . autoloaders : config = self . autoloader_config [ \"JSONLoader\" ] kwargs = { ** config [ \"required\" ], ** config [ \"optional\" ]} try : loader = JSONLoader ( file_path , ** kwargs ) docs = loader . load () except Exception as e : logging . debug ( \"Filepath %s failed to load using JSONLoader: %s \\n \" \"Falling back to generic loader.\" , file_path , e , ) docs = self . fallback_loader ( file_path ) elif file_extension == \"csv\" and \"CSVLoader\" in self . autoloaders : config = self . autoloader_config [ \"CSVLoader\" ] kwargs = { ** config [ \"required\" ], ** config [ \"optional\" ]} try : loader = CSVLoader ( file_path , ** kwargs ) docs = loader . load () except Exception as e : logging . debug ( \"Filepath %s failed to load using CSVLoader: %s \\n \" \"Falling back to generic loader.\" , file_path , e , ) docs = self . fallback_loader ( file_path ) else : # Fallback to unstructured loader. docs = self . fallback_loader ( file_path ) enhanced_docs = [ EnhancedDocument . from_document ( doc ) for doc in docs ] return enhanced_docs def fallback_loader ( self , file_path ) -> List [ Document ]: \"\"\" Uses a generic loader to process files when specific loaders are not applicable or fail. Args: file_path (str): Path to the file being loaded. Returns: List[Document]: A list of Document objects loaded using the fallback method. \"\"\" logging . info ( \"Using fallback loader for %s .\" , file_path ) loader = UnstructuredFileLoader ( file_path , mode = \"elements\" , strategy = \"fast\" , ) docs = loader . load () return docs def unzip_dataset ( self , input_dir : str , unzip_dir : str ) -> str : \"\"\" Unzips a dataset from a specified input directory into a target unzip directory. Args: input_dir (str): Path to the zipped dataset. unzip_dir (str): Target directory for the unzipped files. Returns: str: Path to the directory containing the unzipped files. \"\"\" os . makedirs ( unzip_dir , exist_ok = True ) directory = os . path . join ( unzip_dir , os . path . dirname ( input_dir ), # TODO(STP): Maybe use a helper to remove file extension here. os . path . basename ( input_dir )[: - 4 ], ) unzip_recursively ( input_dir , directory ) return directory def _get_valid_autoloaders ( self ) -> Set [ str ]: \"\"\"Returns the set of valid autoloaders. An autoloader is considered valid if the required arguments for the autoloader exist in the loader_config. This function will only be called once per dataset. Returns: Set[str]: A set of autoloaders that have all required arguments available. \"\"\" autoloaders = set () for autoloader , config in self . autoloader_config . items (): usable = True required_config = config [ \"required\" ] for required_arg , val in required_config . items (): if val is None : usable = False break if usable : autoloaders . add ( autoloader ) return autoloaders __init__ ( autoloader_config = DEFAULT_AUTOLOADER_CONFIG ) \u00b6 Initializes a Loader instance with a given autoloader configuration. Parameters: autoloader_config ( dict , default: DEFAULT_AUTOLOADER_CONFIG ) \u2013 Configuration for autoloaders that determines how different file types are processed. Attributes: autoloader_config ( dict ) \u2013 Stores the provided autoloader configuration. autoloaders ( Set [ str ] ) \u2013 Set of valid autoloaders based on the configuration. Source code in src/load.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , autoloader_config : dict = DEFAULT_AUTOLOADER_CONFIG , ) -> None : \"\"\" Initializes a Loader instance with a given autoloader configuration. Args: autoloader_config (dict): Configuration for autoloaders that determines how different file types are processed. Attributes: autoloader_config (dict): Stores the provided autoloader configuration. autoloaders (Set[str]): Set of valid autoloaders based on the configuration. \"\"\" self . autoloader_config : dict = autoloader_config self . autoloaders : Set [ str ] = self . _get_valid_autoloaders () fallback_loader ( file_path ) \u00b6 Uses a generic loader to process files when specific loaders are not applicable or fail. Parameters: file_path ( str ) \u2013 Path to the file being loaded. Returns: List [ Document ] \u2013 List[Document]: A list of Document objects loaded using the fallback method. Source code in src/load.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def fallback_loader ( self , file_path ) -> List [ Document ]: \"\"\" Uses a generic loader to process files when specific loaders are not applicable or fail. Args: file_path (str): Path to the file being loaded. Returns: List[Document]: A list of Document objects loaded using the fallback method. \"\"\" logging . info ( \"Using fallback loader for %s .\" , file_path ) loader = UnstructuredFileLoader ( file_path , mode = \"elements\" , strategy = \"fast\" , ) docs = loader . load () return docs file_to_docs ( file_path ) \u00b6 Processes a file into a list of EnhancedDocument objects based on the file extension and configured autoloaders. Parameters: file_path ( str ) \u2013 Path to the file being processed. Returns: List [ EnhancedDocument ] \u2013 List[EnhancedDocument]: A list of EnhancedDocument objects created from the file. Source code in src/load.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def file_to_docs ( self , file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Processes a file into a list of EnhancedDocument objects based on the file extension and configured autoloaders. Args: file_path (str): Path to the file being processed. Returns: List[EnhancedDocument]: A list of EnhancedDocument objects created from the file. \"\"\" # NOTE(STP): Switching to unstructured's file-type detection in the # future might be worthwhile (although their check for whether a file # is a JSON file is whether or not json.load() succeeds, which might # not be performant?). # See https://github.com/Unstructured-IO/unstructured/blob/main/unstructured/file_utils/filetype.py # noqa: E501 file_extension = file_path . split ( \".\" )[ - 1 ] if file_extension == \"json\" and \"JSONLoader\" in self . autoloaders : config = self . autoloader_config [ \"JSONLoader\" ] kwargs = { ** config [ \"required\" ], ** config [ \"optional\" ]} try : loader = JSONLoader ( file_path , ** kwargs ) docs = loader . load () except Exception as e : logging . debug ( \"Filepath %s failed to load using JSONLoader: %s \\n \" \"Falling back to generic loader.\" , file_path , e , ) docs = self . fallback_loader ( file_path ) elif file_extension == \"csv\" and \"CSVLoader\" in self . autoloaders : config = self . autoloader_config [ \"CSVLoader\" ] kwargs = { ** config [ \"required\" ], ** config [ \"optional\" ]} try : loader = CSVLoader ( file_path , ** kwargs ) docs = loader . load () except Exception as e : logging . debug ( \"Filepath %s failed to load using CSVLoader: %s \\n \" \"Falling back to generic loader.\" , file_path , e , ) docs = self . fallback_loader ( file_path ) else : # Fallback to unstructured loader. docs = self . fallback_loader ( file_path ) enhanced_docs = [ EnhancedDocument . from_document ( doc ) for doc in docs ] return enhanced_docs load_dataset ( input_dir , output_dir , is_zipped = False , unzip_dir = 'unzipped' , detailed_progress = False , num_workers = 10 , max_files = None ) \u00b6 Loads a dataset from a specified directory, processes files into EnhancedDocument objects, and saves them to disk. Parameters: input_dir ( str ) \u2013 Path to the directory containing the dataset. is_zipped ( bool , default: False ) \u2013 Whether the dataset is in a zipped format. unzip_dir ( str , default: 'unzipped' ) \u2013 Directory to unzip files to, if applicable. output_dir ( str ) \u2013 Directory where processed documents should be saved. detailed_progress ( bool , default: False ) \u2013 Whether to display detailed progress information. num_workers ( int , default: 10 ) \u2013 Number of worker processes to use for loading files. max_files ( int , default: None ) \u2013 Maximum number of files to process. Source code in src/load.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def load_dataset ( self , input_dir : str , output_dir : str , is_zipped : bool = False , unzip_dir : str = \"unzipped\" , detailed_progress : bool = False , num_workers : int = 10 , max_files : Optional [ int ] = None , ) -> None : \"\"\" Loads a dataset from a specified directory, processes files into EnhancedDocument objects, and saves them to disk. Args: input_dir (str): Path to the directory containing the dataset. is_zipped (bool): Whether the dataset is in a zipped format. unzip_dir (str): Directory to unzip files to, if applicable. output_dir (str): Directory where processed documents should be saved. detailed_progress (bool): Whether to display detailed progress information. num_workers (int): Number of worker processes to use for loading files. max_files (int, optional): Maximum number of files to process. \"\"\" logging . debug ( \"Loading dataset from %s \" , input_dir ) if is_zipped : # TODO(STP): Make this check cleaner. dataset_name = os . path . basename ( input_dir )[: - 4 ] if dataset_name [: - 4 ] != \".zip\" : raise ValueError ( \"Zipped dataset name must end in '.zip'. Received dataset \" \"name: %s \" , dataset_name , ) directory = os . path . join ( unzip_dir , dataset_name ) directory = self . unzip_dataset ( input_dir , unzip_dir ) else : directory = input_dir num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( directory ))) partial_func = partial ( self . load_file , save_docs = True , output_dir = output_dir ) with tqdm ( total = num_files , desc = \"Loading files\" , unit = \"files\" , smoothing = 0 ) as pbar : with multiprocessing . Pool ( num_workers ) as pool : for i , _ in enumerate ( pool . imap_unordered ( partial_func , get_files_from_dir ( directory ), ) ): pbar . update ( 1 ) if max_files is not None and i + 1 >= max_files : break load_file ( save_docs , output_dir , file_path ) \u00b6 Loads a single file from the given path and optionally saves the processed document. Parameters: save_docs ( bool ) \u2013 Whether to save the processed documents. output_dir ( str ) \u2013 Directory where processed documents should be saved. file_path ( str ) \u2013 Path to the file being loaded. Raises: AssertionError \u2013 If save_docs is True but output_dir is None. Source code in src/load.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def load_file ( self , save_docs : bool , output_dir : Optional [ str ], file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Loads a single file from the given path and optionally saves the processed document. Args: save_docs (bool): Whether to save the processed documents. output_dir (str, optional): Directory where processed documents should be saved. file_path (str): Path to the file being loaded. Raises: AssertionError: If `save_docs` is True but `output_dir` is None. \"\"\" logging . debug ( \"Loading file: %s \" , file_path ) docs = self . file_to_docs ( file_path ) if save_docs : assert output_dir is not None save_docs_to_file ( docs , file_path , output_dir ) logging . debug ( \"Loaded file: %s \" , file_path ) return docs unzip_dataset ( input_dir , unzip_dir ) \u00b6 Unzips a dataset from a specified input directory into a target unzip directory. Parameters: input_dir ( str ) \u2013 Path to the zipped dataset. unzip_dir ( str ) \u2013 Target directory for the unzipped files. Returns: str ( str ) \u2013 Path to the directory containing the unzipped files. Source code in src/load.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def unzip_dataset ( self , input_dir : str , unzip_dir : str ) -> str : \"\"\" Unzips a dataset from a specified input directory into a target unzip directory. Args: input_dir (str): Path to the zipped dataset. unzip_dir (str): Target directory for the unzipped files. Returns: str: Path to the directory containing the unzipped files. \"\"\" os . makedirs ( unzip_dir , exist_ok = True ) directory = os . path . join ( unzip_dir , os . path . dirname ( input_dir ), # TODO(STP): Maybe use a helper to remove file extension here. os . path . basename ( input_dir )[: - 4 ], ) unzip_recursively ( input_dir , directory ) return directory","title":"Loader"},{"location":"api-reference/load/#loader-api-reference","text":"","title":"Loader API Reference"},{"location":"api-reference/load/#src.load.Loader","text":"Load files into a standardized format. Potential functions to override if implementing a custom Loader class: - file_to_docs() : the logic for how a file is converted to an EnhancedDocument. Source code in src/load.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 class Loader : \"\"\"Load files into a standardized format. Potential functions to override if implementing a custom Loader class: - `file_to_docs()`: the logic for how a file is converted to an EnhancedDocument. \"\"\" def __init__ ( self , autoloader_config : dict = DEFAULT_AUTOLOADER_CONFIG , ) -> None : \"\"\" Initializes a Loader instance with a given autoloader configuration. Args: autoloader_config (dict): Configuration for autoloaders that determines how different file types are processed. Attributes: autoloader_config (dict): Stores the provided autoloader configuration. autoloaders (Set[str]): Set of valid autoloaders based on the configuration. \"\"\" self . autoloader_config : dict = autoloader_config self . autoloaders : Set [ str ] = self . _get_valid_autoloaders () def load_dataset ( self , input_dir : str , output_dir : str , is_zipped : bool = False , unzip_dir : str = \"unzipped\" , detailed_progress : bool = False , num_workers : int = 10 , max_files : Optional [ int ] = None , ) -> None : \"\"\" Loads a dataset from a specified directory, processes files into EnhancedDocument objects, and saves them to disk. Args: input_dir (str): Path to the directory containing the dataset. is_zipped (bool): Whether the dataset is in a zipped format. unzip_dir (str): Directory to unzip files to, if applicable. output_dir (str): Directory where processed documents should be saved. detailed_progress (bool): Whether to display detailed progress information. num_workers (int): Number of worker processes to use for loading files. max_files (int, optional): Maximum number of files to process. \"\"\" logging . debug ( \"Loading dataset from %s \" , input_dir ) if is_zipped : # TODO(STP): Make this check cleaner. dataset_name = os . path . basename ( input_dir )[: - 4 ] if dataset_name [: - 4 ] != \".zip\" : raise ValueError ( \"Zipped dataset name must end in '.zip'. Received dataset \" \"name: %s \" , dataset_name , ) directory = os . path . join ( unzip_dir , dataset_name ) directory = self . unzip_dataset ( input_dir , unzip_dir ) else : directory = input_dir num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( directory ))) partial_func = partial ( self . load_file , save_docs = True , output_dir = output_dir ) with tqdm ( total = num_files , desc = \"Loading files\" , unit = \"files\" , smoothing = 0 ) as pbar : with multiprocessing . Pool ( num_workers ) as pool : for i , _ in enumerate ( pool . imap_unordered ( partial_func , get_files_from_dir ( directory ), ) ): pbar . update ( 1 ) if max_files is not None and i + 1 >= max_files : break def load_file ( self , save_docs : bool , output_dir : Optional [ str ], file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Loads a single file from the given path and optionally saves the processed document. Args: save_docs (bool): Whether to save the processed documents. output_dir (str, optional): Directory where processed documents should be saved. file_path (str): Path to the file being loaded. Raises: AssertionError: If `save_docs` is True but `output_dir` is None. \"\"\" logging . debug ( \"Loading file: %s \" , file_path ) docs = self . file_to_docs ( file_path ) if save_docs : assert output_dir is not None save_docs_to_file ( docs , file_path , output_dir ) logging . debug ( \"Loaded file: %s \" , file_path ) return docs def file_to_docs ( self , file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Processes a file into a list of EnhancedDocument objects based on the file extension and configured autoloaders. Args: file_path (str): Path to the file being processed. Returns: List[EnhancedDocument]: A list of EnhancedDocument objects created from the file. \"\"\" # NOTE(STP): Switching to unstructured's file-type detection in the # future might be worthwhile (although their check for whether a file # is a JSON file is whether or not json.load() succeeds, which might # not be performant?). # See https://github.com/Unstructured-IO/unstructured/blob/main/unstructured/file_utils/filetype.py # noqa: E501 file_extension = file_path . split ( \".\" )[ - 1 ] if file_extension == \"json\" and \"JSONLoader\" in self . autoloaders : config = self . autoloader_config [ \"JSONLoader\" ] kwargs = { ** config [ \"required\" ], ** config [ \"optional\" ]} try : loader = JSONLoader ( file_path , ** kwargs ) docs = loader . load () except Exception as e : logging . debug ( \"Filepath %s failed to load using JSONLoader: %s \\n \" \"Falling back to generic loader.\" , file_path , e , ) docs = self . fallback_loader ( file_path ) elif file_extension == \"csv\" and \"CSVLoader\" in self . autoloaders : config = self . autoloader_config [ \"CSVLoader\" ] kwargs = { ** config [ \"required\" ], ** config [ \"optional\" ]} try : loader = CSVLoader ( file_path , ** kwargs ) docs = loader . load () except Exception as e : logging . debug ( \"Filepath %s failed to load using CSVLoader: %s \\n \" \"Falling back to generic loader.\" , file_path , e , ) docs = self . fallback_loader ( file_path ) else : # Fallback to unstructured loader. docs = self . fallback_loader ( file_path ) enhanced_docs = [ EnhancedDocument . from_document ( doc ) for doc in docs ] return enhanced_docs def fallback_loader ( self , file_path ) -> List [ Document ]: \"\"\" Uses a generic loader to process files when specific loaders are not applicable or fail. Args: file_path (str): Path to the file being loaded. Returns: List[Document]: A list of Document objects loaded using the fallback method. \"\"\" logging . info ( \"Using fallback loader for %s .\" , file_path ) loader = UnstructuredFileLoader ( file_path , mode = \"elements\" , strategy = \"fast\" , ) docs = loader . load () return docs def unzip_dataset ( self , input_dir : str , unzip_dir : str ) -> str : \"\"\" Unzips a dataset from a specified input directory into a target unzip directory. Args: input_dir (str): Path to the zipped dataset. unzip_dir (str): Target directory for the unzipped files. Returns: str: Path to the directory containing the unzipped files. \"\"\" os . makedirs ( unzip_dir , exist_ok = True ) directory = os . path . join ( unzip_dir , os . path . dirname ( input_dir ), # TODO(STP): Maybe use a helper to remove file extension here. os . path . basename ( input_dir )[: - 4 ], ) unzip_recursively ( input_dir , directory ) return directory def _get_valid_autoloaders ( self ) -> Set [ str ]: \"\"\"Returns the set of valid autoloaders. An autoloader is considered valid if the required arguments for the autoloader exist in the loader_config. This function will only be called once per dataset. Returns: Set[str]: A set of autoloaders that have all required arguments available. \"\"\" autoloaders = set () for autoloader , config in self . autoloader_config . items (): usable = True required_config = config [ \"required\" ] for required_arg , val in required_config . items (): if val is None : usable = False break if usable : autoloaders . add ( autoloader ) return autoloaders","title":"Loader"},{"location":"api-reference/load/#src.load.Loader.__init__","text":"Initializes a Loader instance with a given autoloader configuration. Parameters: autoloader_config ( dict , default: DEFAULT_AUTOLOADER_CONFIG ) \u2013 Configuration for autoloaders that determines how different file types are processed. Attributes: autoloader_config ( dict ) \u2013 Stores the provided autoloader configuration. autoloaders ( Set [ str ] ) \u2013 Set of valid autoloaders based on the configuration. Source code in src/load.py 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 def __init__ ( self , autoloader_config : dict = DEFAULT_AUTOLOADER_CONFIG , ) -> None : \"\"\" Initializes a Loader instance with a given autoloader configuration. Args: autoloader_config (dict): Configuration for autoloaders that determines how different file types are processed. Attributes: autoloader_config (dict): Stores the provided autoloader configuration. autoloaders (Set[str]): Set of valid autoloaders based on the configuration. \"\"\" self . autoloader_config : dict = autoloader_config self . autoloaders : Set [ str ] = self . _get_valid_autoloaders ()","title":"__init__"},{"location":"api-reference/load/#src.load.Loader.fallback_loader","text":"Uses a generic loader to process files when specific loaders are not applicable or fail. Parameters: file_path ( str ) \u2013 Path to the file being loaded. Returns: List [ Document ] \u2013 List[Document]: A list of Document objects loaded using the fallback method. Source code in src/load.py 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 def fallback_loader ( self , file_path ) -> List [ Document ]: \"\"\" Uses a generic loader to process files when specific loaders are not applicable or fail. Args: file_path (str): Path to the file being loaded. Returns: List[Document]: A list of Document objects loaded using the fallback method. \"\"\" logging . info ( \"Using fallback loader for %s .\" , file_path ) loader = UnstructuredFileLoader ( file_path , mode = \"elements\" , strategy = \"fast\" , ) docs = loader . load () return docs","title":"fallback_loader"},{"location":"api-reference/load/#src.load.Loader.file_to_docs","text":"Processes a file into a list of EnhancedDocument objects based on the file extension and configured autoloaders. Parameters: file_path ( str ) \u2013 Path to the file being processed. Returns: List [ EnhancedDocument ] \u2013 List[EnhancedDocument]: A list of EnhancedDocument objects created from the file. Source code in src/load.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 def file_to_docs ( self , file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Processes a file into a list of EnhancedDocument objects based on the file extension and configured autoloaders. Args: file_path (str): Path to the file being processed. Returns: List[EnhancedDocument]: A list of EnhancedDocument objects created from the file. \"\"\" # NOTE(STP): Switching to unstructured's file-type detection in the # future might be worthwhile (although their check for whether a file # is a JSON file is whether or not json.load() succeeds, which might # not be performant?). # See https://github.com/Unstructured-IO/unstructured/blob/main/unstructured/file_utils/filetype.py # noqa: E501 file_extension = file_path . split ( \".\" )[ - 1 ] if file_extension == \"json\" and \"JSONLoader\" in self . autoloaders : config = self . autoloader_config [ \"JSONLoader\" ] kwargs = { ** config [ \"required\" ], ** config [ \"optional\" ]} try : loader = JSONLoader ( file_path , ** kwargs ) docs = loader . load () except Exception as e : logging . debug ( \"Filepath %s failed to load using JSONLoader: %s \\n \" \"Falling back to generic loader.\" , file_path , e , ) docs = self . fallback_loader ( file_path ) elif file_extension == \"csv\" and \"CSVLoader\" in self . autoloaders : config = self . autoloader_config [ \"CSVLoader\" ] kwargs = { ** config [ \"required\" ], ** config [ \"optional\" ]} try : loader = CSVLoader ( file_path , ** kwargs ) docs = loader . load () except Exception as e : logging . debug ( \"Filepath %s failed to load using CSVLoader: %s \\n \" \"Falling back to generic loader.\" , file_path , e , ) docs = self . fallback_loader ( file_path ) else : # Fallback to unstructured loader. docs = self . fallback_loader ( file_path ) enhanced_docs = [ EnhancedDocument . from_document ( doc ) for doc in docs ] return enhanced_docs","title":"file_to_docs"},{"location":"api-reference/load/#src.load.Loader.load_dataset","text":"Loads a dataset from a specified directory, processes files into EnhancedDocument objects, and saves them to disk. Parameters: input_dir ( str ) \u2013 Path to the directory containing the dataset. is_zipped ( bool , default: False ) \u2013 Whether the dataset is in a zipped format. unzip_dir ( str , default: 'unzipped' ) \u2013 Directory to unzip files to, if applicable. output_dir ( str ) \u2013 Directory where processed documents should be saved. detailed_progress ( bool , default: False ) \u2013 Whether to display detailed progress information. num_workers ( int , default: 10 ) \u2013 Number of worker processes to use for loading files. max_files ( int , default: None ) \u2013 Maximum number of files to process. Source code in src/load.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 def load_dataset ( self , input_dir : str , output_dir : str , is_zipped : bool = False , unzip_dir : str = \"unzipped\" , detailed_progress : bool = False , num_workers : int = 10 , max_files : Optional [ int ] = None , ) -> None : \"\"\" Loads a dataset from a specified directory, processes files into EnhancedDocument objects, and saves them to disk. Args: input_dir (str): Path to the directory containing the dataset. is_zipped (bool): Whether the dataset is in a zipped format. unzip_dir (str): Directory to unzip files to, if applicable. output_dir (str): Directory where processed documents should be saved. detailed_progress (bool): Whether to display detailed progress information. num_workers (int): Number of worker processes to use for loading files. max_files (int, optional): Maximum number of files to process. \"\"\" logging . debug ( \"Loading dataset from %s \" , input_dir ) if is_zipped : # TODO(STP): Make this check cleaner. dataset_name = os . path . basename ( input_dir )[: - 4 ] if dataset_name [: - 4 ] != \".zip\" : raise ValueError ( \"Zipped dataset name must end in '.zip'. Received dataset \" \"name: %s \" , dataset_name , ) directory = os . path . join ( unzip_dir , dataset_name ) directory = self . unzip_dataset ( input_dir , unzip_dir ) else : directory = input_dir num_files = None if detailed_progress : num_files = len ( list ( get_files_from_dir ( directory ))) partial_func = partial ( self . load_file , save_docs = True , output_dir = output_dir ) with tqdm ( total = num_files , desc = \"Loading files\" , unit = \"files\" , smoothing = 0 ) as pbar : with multiprocessing . Pool ( num_workers ) as pool : for i , _ in enumerate ( pool . imap_unordered ( partial_func , get_files_from_dir ( directory ), ) ): pbar . update ( 1 ) if max_files is not None and i + 1 >= max_files : break","title":"load_dataset"},{"location":"api-reference/load/#src.load.Loader.load_file","text":"Loads a single file from the given path and optionally saves the processed document. Parameters: save_docs ( bool ) \u2013 Whether to save the processed documents. output_dir ( str ) \u2013 Directory where processed documents should be saved. file_path ( str ) \u2013 Path to the file being loaded. Raises: AssertionError \u2013 If save_docs is True but output_dir is None. Source code in src/load.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def load_file ( self , save_docs : bool , output_dir : Optional [ str ], file_path : str ) -> List [ EnhancedDocument ]: \"\"\" Loads a single file from the given path and optionally saves the processed document. Args: save_docs (bool): Whether to save the processed documents. output_dir (str, optional): Directory where processed documents should be saved. file_path (str): Path to the file being loaded. Raises: AssertionError: If `save_docs` is True but `output_dir` is None. \"\"\" logging . debug ( \"Loading file: %s \" , file_path ) docs = self . file_to_docs ( file_path ) if save_docs : assert output_dir is not None save_docs_to_file ( docs , file_path , output_dir ) logging . debug ( \"Loaded file: %s \" , file_path ) return docs","title":"load_file"},{"location":"api-reference/load/#src.load.Loader.unzip_dataset","text":"Unzips a dataset from a specified input directory into a target unzip directory. Parameters: input_dir ( str ) \u2013 Path to the zipped dataset. unzip_dir ( str ) \u2013 Target directory for the unzipped files. Returns: str ( str ) \u2013 Path to the directory containing the unzipped files. Source code in src/load.py 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def unzip_dataset ( self , input_dir : str , unzip_dir : str ) -> str : \"\"\" Unzips a dataset from a specified input directory into a target unzip directory. Args: input_dir (str): Path to the zipped dataset. unzip_dir (str): Target directory for the unzipped files. Returns: str: Path to the directory containing the unzipped files. \"\"\" os . makedirs ( unzip_dir , exist_ok = True ) directory = os . path . join ( unzip_dir , os . path . dirname ( input_dir ), # TODO(STP): Maybe use a helper to remove file extension here. os . path . basename ( input_dir )[: - 4 ], ) unzip_recursively ( input_dir , directory ) return directory","title":"unzip_dataset"},{"location":"user-guide/installation/","text":"Installation \u00b6 Using pip \u00b6 pip install ... From the source \u00b6 git clone ...","title":"Installation"},{"location":"user-guide/installation/#installation","text":"","title":"Installation"},{"location":"user-guide/installation/#using-pip","text":"pip install ...","title":"Using pip"},{"location":"user-guide/installation/#from-the-source","text":"git clone ...","title":"From the source"},{"location":"user-guide/usage/","text":"Usage Guide \u00b6 See the home page for more info on how the design is packaged. Loading a textual dataset into a vectorstore \u00b6 This is the quickest way to ingest a dataset and store the results in a vectorstore. It uses a default loading, chunking, and embedding strategy. This can be done in the following manner: import ... ingester = Ingester() ingester.ingest_dataset( input_dir=\"financial_dataset\", save_intermediate_docs=True, output_dir=\"test_output\", detailed_progress=True, batch_size=100, max_files=1000, ) If you want to specify different configuration options for the default Loader , Chunker , or Embedder , you can do this by instantiating them individually and passing them to the Ingester . Here's an example where we set the configuration options for the provided JSONLoader , The Chunker and Embedder can be overridden in a similar manner. autoloader_config = { \"JSONLoader\": { \"required\": { \"jq_schema\": \".\", }, \"optional\": { \"content_key\": None, \"is_content_key_jq_parsable\": False, \"metadata_func\": None, \"text_content\": True, \"json_lines\": False, }, }, } loader = Loader(autoloader_config) ingester = Ingester(loader=loader) ... Using Custom Classes \u00b6 If you want to include custom logic for how to load files, chunk documents, or embed documents, you can subclass the relevant class and pass it to the Ingester . Each class contains information on which methods should be overriden. Here's an example where a custom loader for loading json documents of a specific format. class CustomLoader(Loader): def file_to_docs(self, file_path: str) -> List[EnhancedDocument]: file_extension = file_path.split(\".\")[-1] if file_extension == \"json\": with open(file_path) as fin: try: data = json.load(fin) text = data[\"text\"] # TODO(STP): Add the filename to the metadata. metadata = {} for key in { \"title\", \"url\", \"site_full\", \"language\", \"published\", }: if key in data: metadata[key] = data[key] if \"source\" in metadata: # HACK(STP): Since source is a reserved keyword for # document metadata, we need to rename it here. metadata[\"source_\"] += metadata[\"source\"] metadata[\"source\"] = file_path return [ EnhancedDocument(page_content=text, metadata=metadata) ] except Exception as e: print(f\"Failed to parse {fin}: {e}. Skipping for now\") return [] else: return super().file_to_docs(file_path)","title":"Usage"},{"location":"user-guide/usage/#usage-guide","text":"See the home page for more info on how the design is packaged.","title":"Usage Guide"},{"location":"user-guide/usage/#loading-a-textual-dataset-into-a-vectorstore","text":"This is the quickest way to ingest a dataset and store the results in a vectorstore. It uses a default loading, chunking, and embedding strategy. This can be done in the following manner: import ... ingester = Ingester() ingester.ingest_dataset( input_dir=\"financial_dataset\", save_intermediate_docs=True, output_dir=\"test_output\", detailed_progress=True, batch_size=100, max_files=1000, ) If you want to specify different configuration options for the default Loader , Chunker , or Embedder , you can do this by instantiating them individually and passing them to the Ingester . Here's an example where we set the configuration options for the provided JSONLoader , The Chunker and Embedder can be overridden in a similar manner. autoloader_config = { \"JSONLoader\": { \"required\": { \"jq_schema\": \".\", }, \"optional\": { \"content_key\": None, \"is_content_key_jq_parsable\": False, \"metadata_func\": None, \"text_content\": True, \"json_lines\": False, }, }, } loader = Loader(autoloader_config) ingester = Ingester(loader=loader) ...","title":"Loading a textual dataset into a vectorstore"},{"location":"user-guide/usage/#using-custom-classes","text":"If you want to include custom logic for how to load files, chunk documents, or embed documents, you can subclass the relevant class and pass it to the Ingester . Each class contains information on which methods should be overriden. Here's an example where a custom loader for loading json documents of a specific format. class CustomLoader(Loader): def file_to_docs(self, file_path: str) -> List[EnhancedDocument]: file_extension = file_path.split(\".\")[-1] if file_extension == \"json\": with open(file_path) as fin: try: data = json.load(fin) text = data[\"text\"] # TODO(STP): Add the filename to the metadata. metadata = {} for key in { \"title\", \"url\", \"site_full\", \"language\", \"published\", }: if key in data: metadata[key] = data[key] if \"source\" in metadata: # HACK(STP): Since source is a reserved keyword for # document metadata, we need to rename it here. metadata[\"source_\"] += metadata[\"source\"] metadata[\"source\"] = file_path return [ EnhancedDocument(page_content=text, metadata=metadata) ] except Exception as e: print(f\"Failed to parse {fin}: {e}. Skipping for now\") return [] else: return super().file_to_docs(file_path)","title":"Using Custom Classes"}]}